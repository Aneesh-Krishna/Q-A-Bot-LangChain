{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Pipeline with Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('Advanced LMS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Advanced LMS.txt'}, page_content=\"Project Idea: Advanced Learning Management System (LMS)\\nDescription: The Advanced Learning Management System (LMS) is a comprehensive platform designed to facilitate modern educational environments by supporting multiple user roles and integrating a variety of collaboration, communication, and learning tools. It goes beyond traditional LMS features to incorporate real-time interaction, multimedia content management, and robust course management functionalities. The LMS ensures a scalable solution that can accommodate institutions of all sizes, offering a centralized system for managing learning activities, student progress, and administrative operations.\\n\\nKey Features:\\nUser Role Management:\\n\\nAdmin Role:\\nFull control over users, courses, and platform operations.\\nManage user roles (admins, teachers, students).\\nMonitor platform metrics, including student performance, video conference attendance, and collaboration usage.\\nTeacher Role:\\nCreate, update, and delete courses.\\nUpload multimedia content such as videos, documents (PDFs, slides), and assignment materials.\\nGrade assignments and provide feedback to students.\\nHost live classes and meetings via integrated video conferencing tools.\\nManage real-time collaboration tools like chat rooms and shared document editing.\\nStudent Role:\\nEnroll in courses.\\nSubmit assignments, receive grades and feedback.\\nParticipate in live video conferencing for classes and discussions.\\nEngage in real-time chat and collaboration tools for group projects.\\nReal-time Collaboration and Communication:\\n\\nIntegrated SignalR-powered live chat feature for student and teacher interaction.\\nDocument sharing and live editing, enabling real-time collaboration on group projects.\\nVideo conferencing for live classes, office hours, or study groups, with scheduling options.\\nMessage notifications and updates for announcements, assignment due dates, or course changes.\\nCourse and Assignment Management:\\n\\nCourse Creation and Material Upload: Teachers can easily create courses, upload lectures, quizzes, and learning materials. Courses can include multimedia content (PDFs, slides, videos).\\nAssignment Submission and Grading: A streamlined system for students to submit assignments, view grades, and receive feedback. Teachers can grade submissions directly on the platform and track progress over time.\\nCourse Enrollment Management: Automatic or manual student enrollment options with course prerequisites and limits.\\nContent Organization: Modules for organizing lessons, materials, and assignments into easily navigable sections.\\nComprehensive Performance Tracking and Analytics:\\n\\nStudent Progress: Detailed dashboards for students and teachers to monitor academic progress, assignment completion, and grades.\\nAnalytics for Admins: Insights into course performance, platform usage, teacher effectiveness, and overall student engagement metrics.\\nGradebook: Teachers can view student grades, attendance, and participation across all courses they manage. Automated grade calculations based on assignments and quizzes.\\nIntegrated Communication Tools:\\n\\nBuilt-in discussion forums for asynchronous course discussions.\\nMessaging system allowing students and teachers to communicate privately.\\nNotifications for important announcements, due dates, and video conferencing invitations.\\nScheduling and Notifications:\\n\\nCalendar Integration: A shared calendar for all users, allowing students and teachers to schedule meetings, classes, and assignment deadlines. Teachers can set up recurring class sessions and office hours.\\nReminder Notifications: Automated reminders for assignment due dates, video conferencing sessions, or upcoming events.\\nAssignment Timers: Real-time tracking of assignments and quizzes with countdown timers visible to students.\\nSecure Authentication and Authorization:\\n\\nASP.NET Identity for handling user authentication and authorization, ensuring secure access for all users.\\nRole-based access control to restrict or grant permissions based on the user role (Admin, Teacher, Student).\\nMulti-factor authentication (MFA) option for enhanced security.\\nVideo Conferencing Integration:\\n\\nTeachers can schedule and host video conferencing directly from the platform, allowing live class sessions or 1-on-1 meetings.\\nIntegrated video chat tool for easy interaction during live sessions.\\nVideo recording feature for classes, allowing students to review lectures at their convenience.\\nMobile-Responsive Design:\\n\\nFully responsive interface that works seamlessly on desktop and mobile devices.\\nMobile-optimized layout for students to access courses, submit assignments, and participate in chats on the go.\\nTechnologies Used:\\nBack-End:\\n\\nC# with ASP.NET Core for scalable, high-performance web services.\\nASP.NET Identity for managing authentication and user roles.\\nSignalR for real-time features such as chat and notifications.\\nEntity Framework Core for database management.\\nDatabase:\\n\\nSQL Server or PostgreSQL to store user data, courses, assignments, and messages securely.\\nFront-End:\\n\\nRazor Pages or React.js for building responsive and interactive UI components.\\nReal-time Communication:\\n\\nSignalR for live chat, collaboration tools, and notifications.\\nExplanation:\\nThis Advanced LMS is designed to cover all essential aspects of managing educational content and interaction in both in-person and remote learning environments. By incorporating real-time collaboration with SignalR, students and teachers can communicate and work together in a dynamic setting, closely mimicking the classroom environment. The platformâ€™s video conferencing and multimedia management allow for a comprehensive online education experience, while the role-based access control ensures that users interact with the platform in a secure, permission-based manner.\\n\\nAdditionally, performance tracking and analytics provide both teachers and administrators valuable insights into student engagement, course efficacy, and areas where intervention may be necessary. This LMS adapts to institutions' needs, offering scalability, secure authentication, and a modern, interactive learning experience for students and teachers alike.\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"OPENAI_API_VERSION\")\n",
    "os.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"OPENAI_API_TYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n      LLM Powered Autonomous Agents\\n    Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Load, chunk and index the content of a html page\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                          bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                            class_=(\"post-title\",\"post-content\",) #Name of the classes of contents to be retrieved\n",
    "                          )))\n",
    "\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024 435\\nCCAG: End-to-End Point Cloud Registration\\nYong Wang, Pengbo Zhou, Guohua Geng,L iA n , and Yangyang Liu\\nAbstract—Point cloud registration is a crucial task in com-\\nputer vision and 3D reconstruction, aiming to align multiple point\\nclouds to achieve globally consistent geometric structures. How-\\never, traditional point cloud registration methods face challenges\\nwhen dealing with low overlap and large-scale point cloud data.\\nTo overcome these issues, we propose an end-to-end point cloud\\nregistration method called CCAG. The CCAG algorithm lever-\\nages the Cross-Convolution Attention module, which combines\\ncross-attention mechanism and depth-wise separable convolution\\nto capture relationships between point clouds and integrate fea-\\ntures. Through cross-attention computation, this module estab-\\nlishes associations between point clouds and utilizes depth-wise\\nseparable convolution operations to extract local features and spa-\\ntial relationships. Furthermore, the CCAG algorithm introduces\\nAdaptive Graph Convolution MLP, which dynamically adjusts\\nnode representations based on the positions of nodes in the graph\\nstructure and features of neighboring nodes, enhancing the expres-\\nsive power of nodes through MLP. Our algorithm demonstrates\\ncompetitive performance in multiple benchmark tests, includ-\\ning 3DMatch/3DLoMatch, KITTI, ModelNet/ModelLoNet, and\\nMVP-RG.\\nIndex Terms—Computer vision, cross-convolution attention,\\nend-to-end, large-scale point cloud data, point cloud registration.\\nI. INTRODUCTION\\nW\\nITH the widespread application of 3D perception and\\npoint cloud data, point cloud registration has attracted\\nsigniﬁcant attention as an important problem in the ﬁelds of\\ncomputer vision and machine learning. Point cloud registration\\naims to align point clouds acquired from different viewpoints\\nor at different times to obtain consistent coordinate systems or\\npose information. It has wide-ranging applications in various\\nﬁelds, including 3D modeling, augmented reality, autonomous\\ndriving, robot perception, and more. However, traditional meth-\\nods for point cloud registration, relying on handcrafted fea-\\nture descriptors and optimization algorithms, struggle to per-\\nform well in scenarios with low overlap and large-scale prob-\\nlems due to the non-structured and incomplete nature of point\\nManuscript received 9 August 2023; accepted 1 November 2023. Date of\\npublication 9 November 2023; date of current version 28 November 2023. This\\nletter was recommended for publication by Associate Editor X. Huang and Editor\\nC. Cadena Lerma upon evaluation of the reviewers’ comments. This work was\\nsupported in part by the National Natural Science Foundation of China under\\nGrant 62271393, and in part by the National Key Research and Development\\nPlan under Grants 2020YFC1523301 and 2020YFC1523303.(Corresponding\\nauthors: Guohua Geng; Li An.)\\nYong Wang, Guohua Geng, Li An, and Yangyang Liu are with the\\nSchool of Information Science and Technology, Northwest University,\\nXian 710127, China (e-mail: 1773943023@qq.com; ghgeng@nwu.edu.cn;\\nanli18394493685@163.com; 513608191@qq.com).\\nPengbo Zhou is with the School of Arts and Communication, Beijing Normal\\nUniversity, Beijing 100875, China (e-mail: fengye883@foxmail.com).\\nDigital Object Identiﬁer 10.1109/LRA.2023.3331666\\ncloud data. Therefore, point cloud registration still faces several\\nchallenges [1].\\nTraditional point cloud registration methods primarily rely\\non manually designed feature descriptors and optimization al-\\ngorithms, such as using geometric properties (e.g., normals,\\ncurvature) of points or handcrafted local feature descriptors.\\nWhile these methods perform well in small-scale and locally\\ndeformable scenarios, their performance is limited when dealing\\nwith complex point cloud data, noise, occlusions, and outliers.\\nIn recent years, the rapid development of deep learning tech-\\nniques has brought new opportunities to point cloud registration.\\nDeep learning methods can automatically learn feature represen-\\ntations of point clouds and perform registration through neural\\nnetworks. Compared to traditional methods, deep learning meth-\\nods possess stronger expressive power and robustness, enabling\\nthem to handle large-scale and complex point cloud data and\\nlearn more discriminative feature representations.\\nBased on deep learning, point cloud registration methods can\\nbe categorized into two types: feature learning and end-to-end.\\nFeature learning methods aim to learn feature representations of\\npoint clouds through deep learning networks and utilize tradi-\\ntional optimization algorithms for registration. These methods\\ncan learn more expressive and robust feature representations\\nfrom point cloud data, leading to improved performance in\\nregistration tasks. End-to-end methods directly learn the trans-\\nformation parameters for registration from raw point cloud data,\\nreducing the complexity of feature extraction and matching\\nwhile providing higher computational efﬁciency and robustness.\\nFurthermore, the correlation between point clouds and the\\nextraction of local features are crucial issues in registration.\\nIn recent years, attention mechanisms and convolution oper-\\nations have gained widespread attention in the ﬁeld of point\\ncloud processing. The combination of attention mechanisms and\\nconvolution operations can enhance the feature modeling and\\ncontext learning capabilities of models, handle spatial locality\\nfeatures, and exhibit strong model generalization. However,\\nchallenges still exist, such as point cloud data regularization\\nand the need for parameter selection and design.\\nIn this letter, we propose an end-to-end point cloud registra-\\ntion method based on CCAG. This method ﬁrst extracts feature\\nrepresentations of input point clouds through sampling and\\nsparsiﬁcation. Then, the feature-extracted point cloud represen-\\ntations are constructed into a graph structure, where the points in\\nthe point clouds serve as nodes in the graph, and the relationships\\nbetween points are treated as edges in the graph. Finally, by\\napplying the Adaptive Graph Convolution MLP module and the\\nCross-Convolution Attention module, information propagation\\nand feature fusion are performed on the nodes of the graph.\\n2377-3766 © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply. '),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='436 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024\\nGraph convolution operations capture global and local point\\ncloud features, while the Cross-Convolution Attention module\\ncomputes the attention distribution between different input point\\nclouds, associating relevant information across different point\\nclouds, learning their dependencies and importance weights, and\\nassisting in matching and aligning different point clouds.\\nThe end-to-end CCAG-based point cloud registration method\\nhas the following advantages:\\nr The AGCM module incorporates shape descriptors and\\nlocal features into the model, addressing the issues of point\\ncloud data regularization, parameter selection, and design,\\nand enhancing the model’s expressive power.\\nr The Cross-Convolution Attention module, by introducing\\ncross-attention computations, models the correlation be-\\ntween point clouds and leverages convolution operations\\nto extract local features, further improving the accuracy\\nand robustness of registration.\\nr We evaluated the proposed method on indoor, outdoor, syn-\\nthetic, and incomplete synthetic datasets, demonstrating\\nstate-of-the-art performance.\\nII. RELATED WORK\\nFeature Extraction Based on Deep Learning:In recent years,\\nwith the rapid development and advancement of deep learning,\\nnew opportunities have emerged for point cloud registration.\\nWang et al.[2] proposed a method using rotation-equivariant\\ndescriptors to address the issue of traditional point cloud regis-\\ntration methods relying on handcrafted feature descriptors. By\\nintroducing rotational equivariance in the design of feature de-\\nscriptors, this method ensures their invariance to rotational trans-\\nformations of point clouds. Speciﬁcally, the authors employed\\nspherical harmonics to represent the local geometric features\\nof point clouds and constructed rotation-equivariant descriptors\\nusing rotation-invariant spherical harmonics. Kadam et al.[3]\\npresented an unsupervised point cloud registration method based\\non R-pointhop. The method ﬁrst applies an adaptive sampling\\nstrategy to downsample the point cloud, reducing computational\\ncomplexity. Then, it computes feature descriptors of the point\\ncloud for local neighborhood matching and initial alignment.\\nNext, an iterative registration method called pointhop is used\\nto progressively improve the alignment accuracy. The pointhop\\nmethod aligns the point cloud iteratively through adaptive fea-\\nture selection and geometry-based strategies until achieving the\\nﬁnal precise registration result.\\nFurthermore, Poiesi et al.[4] proposed an improved method\\nfor point cloud registration by learning general and distinctive\\n3D local depth descriptors. This approach uses deep learning\\nto extract feature representations of point clouds and utilizes\\nthe learned descriptors for matching and registration, thereby\\nachieving more accurate and robust point cloud registration\\nresults. In addressing the balance between accuracy, efﬁciency,\\nand generalization, Ao et al.[5] introduced a point cloud reg-\\nistration method called BUFFER. This method leverages both\\npoint-to-point and face-to-face techniques while overcoming\\ntheir inherent shortcomings. Speciﬁcally, they ﬁrst introduced\\na point-to-point learner to enhance computational efﬁciency\\nand improve feature representation by predicting key points\\nand estimating point orientations. Subsequently, they deployed\\na face-to-face embedder to extract efﬁcient and generic face\\nfeatures using lightweight local feature learners. Different from\\ncommonly used supervised models, Huang et al.[6] proposed an\\ninnovative unsupervised point cloud registration method. They\\nutilized a uniﬁed Gaussian mixture model to handle sampling\\nnoise and density variations, thereby enhancing the accuracy and\\nefﬁciency of point cloud registration while reducing dependence\\non supervised information.\\nEnd-to-end registration based on deep learning:With the\\nrapid development of deep learning, end-to-end point cloud\\nregistration methods based on deep learning have gradually\\nbecome a research hotspot. These methods directly learn the\\ntransformation parameters for registration from raw point cloud\\ndata without explicit feature extraction and matching processes.\\nBy constructing end-to-end neural network models that map\\ninput point cloud data to output transformation parameters,\\naccurate alignment of point clouds can be achieved.\\nTraditional point cloud registration methods often require\\na high degree of overlap between point clouds to establish\\ncorrespondences through matching shared features. However,\\nin low-overlap scenarios, the shared features between point\\nclouds are scarce, making it challenging for traditional meth-\\nods to achieve accurate registration. To address this issue, the\\nPredator method[7] introduced an autoencoder-based approach.\\nThe encoder part of the autoencoder is used to extract feature\\nrepresentations of the point cloud, while the decoder part is\\nresponsible for reconstructing the input point cloud. To han-\\ndle point cloud registration in low-overlap scenarios, Predator\\nincorporates an adaptive point cloud sampling method. By dy-\\nnamically adjusting the point cloud’s sampling density during\\nthe registration process, Predator can better handle point cloud\\nregistration problems in low-overlap situations.\\nOn the other hand, traditional point cloud registration methods\\noften rely on iterative optimization processes to ﬁnd the optimal\\nregistration transformation. However, these iterative optimiza-\\ntion processes often consume a signiﬁcant amount of computa-\\ntion time and require high-quality initial alignment. To address\\nthis problem, Qin et al.[8] proposed a fast and robust point cloud\\nregistration method based on geometric transformations. This\\nmethod ﬁrst computes an initial coarse alignment transformation\\nthrough feature extraction and matching. Then, a geometric\\ntransformation module is utilized to reﬁne the alignment by\\napplying nonlinear transformations based on learnable parame-\\nters, capturing the geometric relationships between point clouds.\\nFinally, through ﬁne alignment and registration evaluation, the\\nﬁnal point cloud registration result is obtained.\\nExisting point cloud descriptors rely on structural information\\nwhile ignoring texture information, yet texture information is\\ncrucial for distinguishing scene components. To address this\\nissue, Huang et al.[9], [10] proposed a method based on multi-\\nmodal fusion to generate point cloud registration descriptors that\\ntake into account both structural and texture information.\\nFurthermore, there are methods that leverage Transformer\\nnetworks to learn end-to-end point cloud correspondences. For\\nexample, Yew et al.[11] proposed a method using Transformer\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply. '),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 2, 'page_label': '3'}, page_content='WANG et al.: CCAG: END-TO-END POINT CLOUD REGISTRATION 437\\nFig. 1. Network architecture of CCAG.\\nnetworks. This method ﬁrst encodes the input point cloud\\nthrough an encoder network, transforming the position and fea-\\nture information of the point cloud into a set of high-dimensional\\nrepresentations. Then, self-attention mechanisms are employed\\nto learn the relationships between points in the point cloud, cap-\\nturing both global and local feature associations in the encoded\\npoint cloud representation. Finally, a fully connected layer maps\\nthe point cloud representation to the prediction space of point\\ncloud correspondences. This prediction space can represent the\\nprobability of correspondence between each point in the point\\ncloud and other points. Based on the predicted correspondences,\\nan iterative optimization algorithm is used to resolve the optimal\\npoint cloud correspondences. Additionally, there are unsuper-\\nvised deep probabilistic methods, such as the method proposed\\nb yM e ie ta l .[12]. This method uses a deep neural network\\nto encode partial input point clouds into low-dimensional fea-\\nture vectors, which are then inputted into a Gaussian mixture\\nmodel. By introducing deep probabilistic models, this method\\ncan automatically learn the matching and registration process\\nfrom partial point clouds in an unsupervised manner.\\nIn summary, deep learning-based point cloud registration\\nmethods offer more accurate, robust, and adaptable solutions\\nfor various registration scenarios through feature learning and\\nend-to-end training. These methods can handle noise and incom-\\npleteness in point clouds and achieve satisfactory registration\\nresults. However, there are still challenges, such as point cloud\\nregistration in large-scale scenes and low-overlap scenarios,\\nwhich remain hotspots and research challenges in the ﬁeld.\\nIII. METHOD\\nOur method follows a hierarchical structure, similar to\\nD3Feat [13] and Predator[7]. The speciﬁc process is illustrated\\nin Fig.1.\\nA. Problem Setting\\nFor two point clouds deﬁned as the source point cloudP =\\n{pi ∈ R3 | i =1 ,2,...,N } and the target point cloud Q =\\n{qi ∈ R3 | i =1 ,2,...,M }, whereN and M are the number\\nof points in point cloudsP and Q, respectively, the goal of\\npoint cloud registration is to align the two point clouds using an\\nFig. 2. Structural ﬂow of the AGCM module and CCA-Net module.\\nunknown 3D rigid transformationRT = {R,T }, which con-\\nsists of a rotationR ∈ SO(3) and a translation T ∈ R3.T h e\\ntransformation matrix can be deﬁned as:\\nmin\\nR,T\\n∑\\n(pi,qi)∈ϑ\\n∥R·pi +T −qi∥2\\n2 (1)\\nWhere ϑ represents the ground truth correspondences between\\nthe points inP and Q. The notation∥•∥ denotes the Euclidean\\nnorm.\\nTo address this problem, we propose an end-to-end point\\ncloud registration algorithm called CCAG, which takes a pair\\nof point clouds as input and output the correspondence between\\npoints, and estimates rigid transformations using RANSAC.\\nB. Downsampling and Feature Extraction\\nFor the denser original point clouds P ∈ RN×3 and Q ∈\\nRM×3, we utilize the KPConv module as the backbone, which\\nconsists of a series of residual modules and strided convolutions,\\nto perform downsampling and reduce the number of keypoints\\nto P′∈ RN′×3 and Q′∈ RM′×3, respectively (whereN>N ′\\nand M>M ′). Furthermore, we employ a shared encoding\\nmethod to extract relevant features, resulting inF′\\nP′ ∈ RN′×D\\nand F′\\nQ′ ∈ RM′×D, whereD represents the feature dimension.\\nC. Overlap Attention Module\\nThe structural ﬂow of the AGCM module and CCA-Net\\nmodule in the Overlapping Attention Module is shown in Fig.2.\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply. '),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 3, 'page_label': '4'}, page_content='438 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024\\nAdaptive Graph Convolution MLP:To address the irregularity\\nof point cloud data and the challenges in selecting and designing\\nnetwork structure parameters, we propose an Adaptive Graph\\nConvolutional MLP network, referred to as AGCM. Since the\\nprocessing steps for the source point cloud and the target point\\ncloud are identical, we will use the source point cloud as an\\nexample.\\nFirstly, we deﬁne the graph structureG(V,E ), where V =\\n{1,2,...,N ′}represents the set of vertices andE ⊆| V|×| V|\\nrepresents the set of edges, with each vertex corresponding to a\\npoint p′\\ni in P′.\\nNext, through the adaptive graph convolution operation, the\\nfeature information of nodes is dynamically aggregated and\\ntransformed. This operation utilizes the feature information of\\nneighboring nodes to update the feature representation of the\\ncurrent node. Then, the maxpooling operation is employed to\\nextract key features. Finally, the MLP enhancement algorithm\\nis introduced to enhance the expressiveness of point cloud data\\nand enrich the features.\\n(k+1)P′= ϑm(max(Δpij ·Δfij)) (2)\\nFAGCM\\ni = γθ(cat[(0)P′,(1)P′,(2)P′]) (3)\\nWhere, Δpij =[ p′\\ni,p′\\nj −p′\\ni] is the graph vertex and its relative\\nposition, Δfij is deﬁned as[f′\\ni,f ′\\nj −f′\\ni] correspondingly, and\\ncat[•,•]is the concatenation operation.ϑm stands for pointwise\\nMLP andγθ stands for linear layer.maxstands for maxpooling.\\nCross-Convolution Attention: To enhance the feature mod-\\neling capability, context relationship learning, handling spatial\\nlocality features, and possessing strong model generalization\\nability, we propose a Cross-Convolution Attention module, re-\\nferred to as CCA-Net.\\nFirstly, we deﬁne a 4-layer multi-head cross-attention module\\nthat inherits the local feature information from the AGCM\\nmodule and focuses on the mutual information between two\\npoint clouds.\\nMHAttn(Q,K,V )=( Head1 ⊕···⊕ HeadH)Wo (4)\\nHeadH = Attn(QWQ\\nh ,KW K\\nh ,VW V\\nh ) (5)\\nWhere, WQ\\nh ∈ Rd×dQ,W K\\nh ∈ Rd×dK ,W V\\nh ∈ Rd×dV ,W O ∈\\nRdHead×d are the learning transformation matrix,dQ,dK,dV\\nare the dimensions of Query, Key and Value,dQ = dK = dV =\\ndHead = D/H. H is the number of heads.\\nFirstly, we deﬁne F′\\nP′ =( xP′\\n1 ,xP′\\n2 ··· xP′\\nN′) and F′\\nQ′ =\\n(xQ′\\n1 ,xQ′\\n2 ··· xQ′\\nN′) as the inputMHAttn(F′\\nP′,F ′\\nQ′,F ′\\nQ′) of the\\ncross-attention module in the i-th layer, Z′′=( zP′,\\n1\\nQ′,zP′,Q′\\n2 ··· zP′,Q′\\nN′ ) is the output matrix, and its formula\\nis as follows:\\nzP′,Q′\\ni =\\nN′\\n∑\\nj=1\\nsoft max(αCross−\\ni,j )xQ′\\nj WV,Q′\\n(6)\\nWhere, αCross−\\ni,j is the weight coefﬁcient that has not been\\nnormalized, and its deﬁnition is as follows:\\nαCross−\\ni,j = 1√dhead\\n(xQ′\\ni WQ,Q′\\n)(xP′\\nj WK,P′\\n)T (7)\\nNext, convolution is used to further capture the local and\\nglobal features of the point cloud. Its deﬁnition is as follows:\\nFinally, the co-global context information between the two\\npoint clouds is output, which is deﬁned as follows:\\nFDSConv\\ni = F′\\nP′ +DSConv(zP′,Q′\\ni ) (8)\\nWhere, DSConv represents depthwise separable convolution,\\nwhich consists of two components: Depthwise Convolution and\\nPointwise Convolution. DSConv=PWConv (DepConv(•)), De-\\npConv, PWConv represent depthwise convolution and pointwise\\nconvolution respectively.\\nFCCA−Net\\ni = FDSConv\\ni +MLP(FDSConv\\ni ) (9)\\nD. Decoder\\nThe module is conﬁgured in the usual manner, with a 3-layer\\nnetwork structure that mainly includes upsampling, linear trans-\\nformation, and skip connections.\\nE. Loss Function\\nThe CCAG network we proposed is trained end-to-end and\\nsupervised using ground truth. The speciﬁc loss function is as\\nfollows:\\nFeature Loss:Similar to the D3Feat and Predator approaches,\\nwe use a circle loss function to evaluate the feature loss and\\nconstrain the point-wise feature descriptors during the training\\nof 3D point clouds. The circle loss function is deﬁned as follows:\\nLP\\nFL = 1\\nNP\\nNP∑\\ni=1\\nlog\\n⎡\\n⎣1+\\n∑\\nj∈εp\\necβj\\np(dj\\ni−Δp) •\\n∑\\nk∈εn\\neλβk\\np(Δn−dk\\ni )\\n⎤\\n⎦\\n(10)\\nWhere, dj\\ni represents the Euclidean distance between features,\\ndj\\ni = ∥fpi −fqj ∥2. εp and εn respectively represent the match-\\ning and unmatching points of the point setPRS (random sam-\\npling points of the source point cloud), i.e., the positive and neg-\\native areas.Δpand Δnrepresent positive and negative regions,\\nrespectively. λ stands for predeﬁned parameters. Similarly, the\\nfeature lossLP\\nFL of the target point cloud is also calculated in\\nthe same way, so the total feature lossLFL = 1\\n2(LP\\nFL +LQ\\nFL).\\nOverlap Loss: We use a binary cross-entropy loss function\\nfor supervised training, which is deﬁned as follows:\\nLP\\nOL = 1\\nN\\nN∑\\ni=1\\nOlabel\\npi log(Opi)+(1 −Olabel\\npi )log(1 −Opi)\\n(11)\\nWhere, Olabel\\npi represents the overlap scores of ground truth at\\npoint pi, which is deﬁned as follows:\\nOlabel\\npi =\\n{1,\\n\\ued79\\ued79TGT\\nP,Q(pi)−NN(TGT\\nP,Q(pi),Q)\\n\\ued79\\ued79<τ 1\\n0,otherwise (12)\\nWhere TGT\\nP,Q represents the ground truth rigid transformation\\nbetween the overlapping point clouds, andNN represents the\\nnearest neighbor.τ1 represents the overlap threshold. Similarly,\\nthe overlap lossLQ\\nOL of the target point cloud is calculated in\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply. '),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 4, 'page_label': '5'}, page_content='WANG et al.: CCAG: END-TO-END POINT CLOUD REGISTRATION 439\\nthe same way. Therefore, the total overlap loss is deﬁned as\\nLOL = 1\\n2(LP\\nOL +LQ\\nOL).\\nTo sum up, the overall loss function isL = LFL +LOL.\\nIV . EXPERIMENTS\\nA. Implementation Details and Evaluation Metrics\\nOur method is implemented using PyTorch[14] and trained\\non an Nvidia RTX 4090, Intel(R) Core(TM) i7-13700KF CPU\\n@ 3.40 GHz, 128 GB RAM.\\nFollowing the evaluation metrics used in Predator [7],\\nREGTR [11], and GMCNet [15], we evaluate our models\\non the datasets using Registration Recall (RR), Relative Ro-\\ntation Error (RRE), and Relative Translation Error (RTE).\\nIn addition, we use the modiﬁed Chamfer Distance (CD) to\\nevaluate the ModelNet40 dataset, and RMSE to evaluate the\\nMVP-RG dataset. The deﬁnitions of RRE, RTE, CD are as\\nfollows:\\nRTE =\\n\\ued79\\ued79t−tGT\\ued79\\ued79\\n2 (13)\\nRRE = arccos\\n(trace(RTRGT)−1\\n2\\n)\\n(14)\\nCD(P,Q)= 1\\n|P|\\n∑\\np∈P\\nmin\\nq∈Qraw\\n\\ued79\\ued79TGT\\nP,Q(p)−q\\n\\ued79\\ued792\\n2 (15)\\n+ 1\\n|Q|\\n∑\\nq∈Q\\nmin\\np∈Praw\\n\\ued79\\ued79q −TGT\\nP,Q(p)\\n\\ued79\\ued792\\n2 (16)\\nWhere, tGT and RGT represent the ground-truth rotational error\\nand translational error, respectively.\\nRegistration recall is deﬁned as the root mean square error of\\nthe transformation being less than 0.2 m. The formula is shown\\nbelow:\\nRMSE =\\n\\ued6a\\ued6b\\ued6b√ 1⏐⏐CGT\\nij\\n⏐⏐\\n∑\\n(p,q)∈CGT\\nij\\n\\ued79\\ued79\\ued79TGT\\nP,Q(p)−q\\n\\ued79\\ued79\\ued79\\n2\\n2\\n(17)\\nWhere, CGT\\nij represents the set of ground-truth correspondences.\\nNote that, unlike the RMSE in registration recall, where only\\nmatched points are considered, in the MVP-RG dataset, all\\npoints are involved in the computation. Therefore, to differenti-\\nate it from (18), we deﬁne the RMSE for the MVP-RG dataset\\nas follows:\\nLRMSE = 1\\nN\\nN∑\\ni=1\\n\\ued79\\ued79TGT(pi)−T(pi)\\n\\ued79\\ued79\\n2 (18)\\nB. 3DMatch and 3DLoMatch\\nDataset: The 3DMatch dataset contains real indoor data from\\n62 scenarios, with 46 scenes for training, 8 for validation, and\\n8 for testing. We ﬁrst pretrain our model using the Predator\\nmethod and then evaluate it on the 3DMatch and 3DLoMatch\\ndatasets. The overlapping areas of the 3DMatch and 3DLoMatch\\ndatasets are greater than 30% and between 10% and 30%,\\nrespectively.\\nTABLE I\\nPERFORMANCE ON 3DMATCH AND3DLOMATCH DATASETS\\nFig. 3. Registration visualization on 3DMatch, 3DLoMatch.\\nBaselines: We compare our results with state-of-the-art meth-\\nods such as FCGF[16], D3Feat[13], Predator[7],O M N e t[17],\\nCoFiNet [18], GeoTrans[8], REGTR[11], UDPReg[12], Lep-\\nard [19],M A C[20] and SC2 -PCR++[21].\\nRegistration Results:The performance comparison of differ-\\nent algorithms is shown in TableI, where the best-performing al-\\ngorithms are indicated in bold. The data in the table are obtained\\nfrom REGTR[11] and UDPReg[12]. Additionally, Fig.3 illus-\\ntrates the registration results on the 3DMatch and 3DLoMatch\\ndatasets. In the testing on the 3DMatch and 3DLoMatch datasets,\\nour algorithm has demonstrated overall superior performance.\\nIt is noteworthy that in low-overlap validation, our algorithm\\nhas shown a remarkable improvement in RR compared to the\\nMAC [20] and SC2 -PCR++[21] algorithms, with increases of\\n16.3% and 15.1%, respectively.\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply. '),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 5, 'page_label': '6'}, page_content='440 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024\\nTABLE II\\nFMR ON 3DMATCH AND3DLOMATCH DATASETS\\nTABLE III\\nPERFORMANCE ON ODOMETRY KITTI DATASET\\nFollowing Predator [7], we evaluated the Feature Match-\\ning Recall (FMR). In the validation experiments presented in\\nTable II , our algorithm demonstrated overall strong perfor-\\nmance.\\nC. Odometry KITTI\\nDataset: The Odometry KITTI dataset consists of large-scale\\nLiDAR scanning data from 11 scenarios, with scenarios 0–5\\nused for training, scenarios 6–7 for validation, and scenarios\\n8–10 for testing.\\nBaselines: We select FCGF[16], D3Feat[13], Predator[7],\\nCoFiNet [18], GeoTrans[8],M A C[20] and SC2 -PCR++[21]\\nas our baseline algorithms.\\nRegistration Results:The comparison of different algorithms\\ni ss h o w ni nT a b l eIII, where the best-performing algorithms are\\nindicated in bold. Additionally, Fig.4 illustrates the registration\\nresults on the KITTI Odometry dataset. In the testing on the\\nKITTI Odometry dataset, our algorithm achieves the highest\\naverage registration recall. Moreover, we also achieve the lowest\\nRTE and RRE scores.\\nD. Modelnet\\nData: The ModelNet40 dataset is a synthetic dataset com-\\nposed of computer-aided design (CAD) models, consisting of\\n5112 samples for training, 1202 samples for validation, and 1266\\nsamples for testing. We follow the training approach of RPM-Net\\nand Predator and then evaluate our model on the ModelNet40\\nand ModelLoNet40 datasets. (The average overlap regions for\\nModelNet40 and ModelLoNet40 datasets are greater than 73.5%\\nand 53.6%, respectively.)\\nFig. 4. Registration visualization on Odometry KITTI.\\nTABLE IV\\nPERFORMANCE ON MODELNET AND MODELLONET DATASETS\\nBaselines: Based on the results of RPM-Net and Predator,\\nwe select several state-of-the-art algorithms, including RPM-\\nNet [24], Predator [7],O M N e t[17], REGTR [11] and UD-\\nPReg [12] as our baselines.\\nRegistration Results:The comparison of different algorithms\\nis shown in Table IV, where the best-performing algorithm\\nis indicated in bold. The data for the table is sourced from\\nREGTR. Additionally, Fig. 5 presents the registration results\\non the ModelNet/ModelLoNet datasets. Our algorithm exhibits\\ngood performance in various evaluations during the testing of\\nthe ModelNet and ModelLoNet datasets.\\nE. MVP-RG\\nData: The MVP-RG dataset is a synthetic dataset of incom-\\nplete point clouds, consisting of 6400 pairs for training and 1200\\npairs for testing. We trained our model using an approach similar\\nto Predator and evaluated it on the MVP-RG dataset.\\nBaselines: We selected several state-of-the-art algorithms,\\nincluding DCP[25], RPM-Net[24],G M C N e t[15],I D A M[26],\\nDeepGMR [27], Predator and DSMNet[28] as our baselines.\\nRegistration Results:The comparison of different algorithms\\nis shown in TableV, where the best performing algorithm is\\nhighlighted in bold, and the data is sourced from GMCNet[15].\\nAdditionally, Fig. 6 illustrates the registration results on the\\nMVP-RG dataset. In the testing phase on the MVP-RG dataset,\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply. '),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 6, 'page_label': '7'}, page_content='WANG et al.: CCAG: END-TO-END POINT CLOUD REGISTRATION 441\\nFig. 5. Registration visualization on ModelNet and ModelLoNet.\\nTABLE V\\nPERFORMANCE ON MVP-RG DATASET\\nour algorithm outperforms other methods in all evaluation met-\\nrics.\\nF . Ablation Study\\nImportance of Individual Modules:To validate the effective-\\nness of selected modules in our model, we conducted ablation\\nexperiments on the 3DMatch/3DLoMatch datasets.\\nFrom Table VI, it can be seen that compared to the other\\nfour individual modules, our proposed CCAG algorithm demon-\\nstrates the best performance.\\nLoss Functions: To evaluate the impact of different loss\\nfunctions and their combinations on the model, we conducted\\nexperiments on the 3DMatch/3DLoMatch datasets. Here, OL\\nrepresents the overlap loss, FL represents the feature loss, and\\nML represents the matching loss.\\nFrom TableVII, it can be observed that all three loss functions\\ncan improve the registration accuracy, with the best performance\\nFig. 6. Registration visualization on MVP-RG.\\nTABLE VI\\nABLATION OFDIFFERENT MODULES ON THE3DMATCH,3 DLOMATCH\\nDATASET\\nTABLE VII\\nABLATION OFDIFFERENT LOSS FUNCTIONS ON THE3DMATCH,\\n3DLOMATCH DATASET\\nachieved when the three loss functions are combined. Among\\nthem, in terms of single loss functions and pairwise combina-\\ntions, the feature loss function or the combination involving the\\nfeature loss function shows the best performance. The overlap\\nloss function performs well, while the matching loss function\\nshows relatively poorer performance.\\nV. CONCLUSION\\nIn response to the non-structural and incomplete nature of\\npoint clouds, as well as the challenges faced by traditional point\\ncloud registration methods in handling low overlap and large-\\nscale data, we propose an end-to-end point cloud registration\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply. '),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='442 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024\\nalgorithm based on CCAG. In the CCAG algorithm, the AGCM\\nmodule converts point clouds into graph structures and adap-\\ntively learns convolutional kernels based on the characteristics\\nof point clouds, enabling effective capture of local and global\\nfeatures. The CCA-Net module introduces depth-wise separa-\\nble convolution and cross-convolution attention computation to\\nmodel the correlations between point clouds and extract local\\nfeatures of overlapping point clouds, thereby further improving\\nthe accuracy and robustness of registration.\\nCompared to traditional registration methods, our algorithm\\ncan better handle the geometric structure and local features of\\npoint clouds, providing more accurate results for point cloud\\nregistration tasks. This approach offers an effective solution for\\npoint cloud processing and the ﬁeld of computer vision in point\\ncloud registration tasks.\\nREFERENCES\\n[1] J. Li, P. Shi, Q. Hu, and Y . Zhang, “QGORE: Quadratic-time guaranteed\\noutlier removal for point cloud registration,”IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 45, no. 9, pp. 11136–11151, Sep. 2023.\\n[2] H. Wang, Y . Liu, Z. Dong, and W. Wang, “You only hypothesize once:\\nPoint cloud registration with rotation-equivariant descriptors,” inProc.\\n30th ACM Int. Conf. Multimedia, 2022, pp. 1630–1641.\\n[3] P. Kadam, M. Zhang, S. Liu, and C.-C. J. Kuo, “R-pointhop: A green,\\naccurate, and unsupervised point cloud registration method,”IEEE Trans.\\nImage Process., vol. 31, pp. 2710–2725, 2022.\\n[4] F. Poiesi and D. Boscaini, “Learning general and distinctive 3D local deep\\ndescriptors for point cloud registration,”IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 45, no. 3, pp. 3979–3985, Mar. 2023.\\n[5] S. Ao, Q. Hu, H. Wang, K. Xu, and Y . Guo, “Buffer: Balancing accu-\\nracy, efﬁciency, and generalizability in point cloud registration,” inProc.\\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2023, pp. 1255–1264.\\n[6] X. Huang, S. Li, Y . Zuo, Y . Fang, J. Zhang, and X. Zhao, “Unsupervised\\npoint cloud registration by learning uniﬁed Gaussian mixture models,”\\nIEEE Robot. Automat. Lett., vol. 7, no. 3, pp. 7028–7035, Jul. 2022.\\n[7] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, and K. Schindler, “Predator:\\nRegistration of 3D point clouds with low overlap,” inProc. IEEE/CVF\\nConf. Comput. Vis. Pattern Recognit., 2021, pp. 4267–4276.\\n[8] Z. Qin, H. Yu, C. Wang, Y . Guo, Y . Peng, and K. Xu, “Geometric trans-\\nformer for fast and robust point cloud registration,” inProc. IEEE/CVF\\nConf. Comput. Vis. Pattern Recognit., 2022, pp. 11143–11152.\\n[9] X. Huang, W. Qu, Y . Zuo, Y . Fang, and X. Zhao, “IMFNet: Interpretable\\nmultimodal fusion for point cloud registration,”IEEE Robot. Automat.\\nLett., vol. 7, no. 4, pp. 12323–12330, Oct. 2022.\\n[10] M. Yuan, X. Huang, K. Fu, Z. Li, and M. Wang, “Boosting 3D point cloud\\nregistration by transferring multi-modality knowledge,” inProc. IEEE Int.\\nConf. Robot. Automat., 2023, pp. 11734–11741.\\n[11] Z. J. Yew and G. H. Lee, “RegTR: End-to-end point cloud correspon-\\ndences with transformers,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\\nRecognit., 2022, pp. 6677–6686.\\n[12] G. Mei et al., “Unsupervised deep probabilistic approach for partial\\npoint cloud registration,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\\nRecognit., 2023, pp. 13611–13620.\\n[13] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, “D3Feat: Joint\\nlearning of dense detection and description of 3D local features,” inProc.\\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 6359–6367.\\n[14] A. Paszke et al., “PyTorch: An imperative style, high-performance deep\\nlearning library,” inProc. 33rd Int. Conf. Neural Inf. Process. Syst., 2019,\\npp. 8026–8037.\\n[15] L. Pan, Z. Cai, and Z. Liu, “Robust partial-to-partial point cloud registra-\\ntion in a full range,” 2021,arXiv:2111.15606.\\n[16] C. Choy, J. Park, and V . Koltun, “Fully convolutional geometric features,”\\nin Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 8958–8966.\\n[17] H. Xu, S. Liu, G. Wang, G. Liu, and B. Zeng, “OMNet: Learning\\noverlapping mask for partial-to-partial point cloud registration,” inProc.\\nIEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 3132–3141.\\n[18] H. Yu, F. Li, M. Saleh, B. Busam, and S. Ilic, “CoFinet: Reliable coarse-\\nto-ﬁne correspondences for robust point cloud registration,” inProc. Adv.\\nNeural Inf. Process. Syst., 2021, pp. 23872–23884.\\n[19] Y . Li and T. Harada, “Lepard: Learning partial point cloud matching in rigid\\nand deformable scenes,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\\nRecognit., 2022, pp. 5554–5564.\\n[20] X. Zhang, J. Yang, S. Zhang, and Y . Zhang, “3D registration with maximal\\ncliques,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2023,\\npp. 17745–17754.\\n[21] Z. Chen, K. Sun, F. Yang, L. Guo, and W. Tao, “SC2-PCr : Rethinking the\\ngeneration and selection for efﬁcient and robust point cloud registration,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 10, pp. 12358–12376,\\nOct. 2023.\\n[22] H. Yu et al., “Rotation-invariant transformer for point cloud match-\\ning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2023,\\npp. 5384–5393.\\n[23] G. Mei, X. Huang, L. Yu, J. Zhang, and M. Bennamoun, “COTReg:\\nCoupled optimal transport based point cloud registration,” 2021,\\narXiv:2112.14381.\\n[24] Z. J. Yew and G. H. Lee, “RPM-Net: Robust point matching using learned\\nfeatures,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020,\\npp. 11824–11833.\\n[25] Y . Wang and J. M. Solomon, “Deep closest point: Learning representations\\nfor point cloud registration,” inProc. IEEE/CVF Int. Conf. Comput. Vis.,\\n2019, pp. 3523–3532.\\n[26] J. Li, C. Zhang, Z. Xu, H. Zhou, and C. Zhang, “Iterative distance-aware\\nsimilarity matrix convolution with mutual-supervised point elimination\\nfor efﬁcient point cloud registration,” inProc. Comput. Vis. ECCV: 16th\\nEur. Conf., 2020, pp. 378–394.\\n[27] W. Yuan, B. Eckart, K. Kim, V . Jampani, D. Fox, and J. Kautz, “Deep-\\nGMR: Learning latent Gaussian mixture models for registration,” inProc.\\nComput. Vis., 16th Eur. Conf., 2020, pp. 733–750.\\n[28] C. Qiu, Z. Wang, X. Lin, Y . Zang, C. Wang, and W. Liu, “DSMNet: Deep\\nhigh-precision 3D surface modeling from sparse point cloud frames,”IEEE\\nGeosci. Remote Sens. Lett., vol. 20, pp. 1–5, 2023.\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply. ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pdf reader (Including images in the pdf)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('CCAG_End-to-End_Point_Cloud_Registration.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024 435\\nCCAG: End-to-End Point Cloud Registration\\nYong Wang, Pengbo Zhou, Guohua Geng,L iA n , and Yangyang Liu\\nAbstract—Point cloud registration is a crucial task in com-\\nputer vision and 3D reconstruction, aiming to align multiple point\\nclouds to achieve globally consistent geometric structures. How-\\never, traditional point cloud registration methods face challenges\\nwhen dealing with low overlap and large-scale point cloud data.\\nTo overcome these issues, we propose an end-to-end point cloud\\nregistration method called CCAG. The CCAG algorithm lever-\\nages the Cross-Convolution Attention module, which combines\\ncross-attention mechanism and depth-wise separable convolution\\nto capture relationships between point clouds and integrate fea-\\ntures. Through cross-attention computation, this module estab-\\nlishes associations between point clouds and utilizes depth-wise'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='to capture relationships between point clouds and integrate fea-\\ntures. Through cross-attention computation, this module estab-\\nlishes associations between point clouds and utilizes depth-wise\\nseparable convolution operations to extract local features and spa-\\ntial relationships. Furthermore, the CCAG algorithm introduces\\nAdaptive Graph Convolution MLP, which dynamically adjusts\\nnode representations based on the positions of nodes in the graph\\nstructure and features of neighboring nodes, enhancing the expres-\\nsive power of nodes through MLP. Our algorithm demonstrates\\ncompetitive performance in multiple benchmark tests, includ-\\ning 3DMatch/3DLoMatch, KITTI, ModelNet/ModelLoNet, and\\nMVP-RG.\\nIndex Terms—Computer vision, cross-convolution attention,\\nend-to-end, large-scale point cloud data, point cloud registration.\\nI. INTRODUCTION\\nW\\nITH the widespread application of 3D perception and\\npoint cloud data, point cloud registration has attracted'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='end-to-end, large-scale point cloud data, point cloud registration.\\nI. INTRODUCTION\\nW\\nITH the widespread application of 3D perception and\\npoint cloud data, point cloud registration has attracted\\nsigniﬁcant attention as an important problem in the ﬁelds of\\ncomputer vision and machine learning. Point cloud registration\\naims to align point clouds acquired from different viewpoints\\nor at different times to obtain consistent coordinate systems or\\npose information. It has wide-ranging applications in various\\nﬁelds, including 3D modeling, augmented reality, autonomous\\ndriving, robot perception, and more. However, traditional meth-\\nods for point cloud registration, relying on handcrafted fea-\\nture descriptors and optimization algorithms, struggle to per-\\nform well in scenarios with low overlap and large-scale prob-\\nlems due to the non-structured and incomplete nature of point\\nManuscript received 9 August 2023; accepted 1 November 2023. Date of'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='form well in scenarios with low overlap and large-scale prob-\\nlems due to the non-structured and incomplete nature of point\\nManuscript received 9 August 2023; accepted 1 November 2023. Date of\\npublication 9 November 2023; date of current version 28 November 2023. This\\nletter was recommended for publication by Associate Editor X. Huang and Editor\\nC. Cadena Lerma upon evaluation of the reviewers’ comments. This work was\\nsupported in part by the National Natural Science Foundation of China under\\nGrant 62271393, and in part by the National Key Research and Development\\nPlan under Grants 2020YFC1523301 and 2020YFC1523303.(Corresponding\\nauthors: Guohua Geng; Li An.)\\nYong Wang, Guohua Geng, Li An, and Yangyang Liu are with the\\nSchool of Information Science and Technology, Northwest University,\\nXian 710127, China (e-mail: 1773943023@qq.com; ghgeng@nwu.edu.cn;\\nanli18394493685@163.com; 513608191@qq.com).\\nPengbo Zhou is with the School of Arts and Communication, Beijing Normal'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='Xian 710127, China (e-mail: 1773943023@qq.com; ghgeng@nwu.edu.cn;\\nanli18394493685@163.com; 513608191@qq.com).\\nPengbo Zhou is with the School of Arts and Communication, Beijing Normal\\nUniversity, Beijing 100875, China (e-mail: fengye883@foxmail.com).\\nDigital Object Identiﬁer 10.1109/LRA.2023.3331666\\ncloud data. Therefore, point cloud registration still faces several\\nchallenges [1].\\nTraditional point cloud registration methods primarily rely\\non manually designed feature descriptors and optimization al-\\ngorithms, such as using geometric properties (e.g., normals,\\ncurvature) of points or handcrafted local feature descriptors.\\nWhile these methods perform well in small-scale and locally\\ndeformable scenarios, their performance is limited when dealing\\nwith complex point cloud data, noise, occlusions, and outliers.\\nIn recent years, the rapid development of deep learning tech-\\nniques has brought new opportunities to point cloud registration.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='with complex point cloud data, noise, occlusions, and outliers.\\nIn recent years, the rapid development of deep learning tech-\\nniques has brought new opportunities to point cloud registration.\\nDeep learning methods can automatically learn feature represen-\\ntations of point clouds and perform registration through neural\\nnetworks. Compared to traditional methods, deep learning meth-\\nods possess stronger expressive power and robustness, enabling\\nthem to handle large-scale and complex point cloud data and\\nlearn more discriminative feature representations.\\nBased on deep learning, point cloud registration methods can\\nbe categorized into two types: feature learning and end-to-end.\\nFeature learning methods aim to learn feature representations of\\npoint clouds through deep learning networks and utilize tradi-\\ntional optimization algorithms for registration. These methods\\ncan learn more expressive and robust feature representations\\nfrom point cloud data, leading to improved performance in'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='tional optimization algorithms for registration. These methods\\ncan learn more expressive and robust feature representations\\nfrom point cloud data, leading to improved performance in\\nregistration tasks. End-to-end methods directly learn the trans-\\nformation parameters for registration from raw point cloud data,\\nreducing the complexity of feature extraction and matching\\nwhile providing higher computational efﬁciency and robustness.\\nFurthermore, the correlation between point clouds and the\\nextraction of local features are crucial issues in registration.\\nIn recent years, attention mechanisms and convolution oper-\\nations have gained widespread attention in the ﬁeld of point\\ncloud processing. The combination of attention mechanisms and\\nconvolution operations can enhance the feature modeling and\\ncontext learning capabilities of models, handle spatial locality\\nfeatures, and exhibit strong model generalization. However,\\nchallenges still exist, such as point cloud data regularization'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='context learning capabilities of models, handle spatial locality\\nfeatures, and exhibit strong model generalization. However,\\nchallenges still exist, such as point cloud data regularization\\nand the need for parameter selection and design.\\nIn this letter, we propose an end-to-end point cloud registra-\\ntion method based on CCAG. This method ﬁrst extracts feature\\nrepresentations of input point clouds through sampling and\\nsparsiﬁcation. Then, the feature-extracted point cloud represen-\\ntations are constructed into a graph structure, where the points in\\nthe point clouds serve as nodes in the graph, and the relationships\\nbetween points are treated as edges in the graph. Finally, by\\napplying the Adaptive Graph Convolution MLP module and the\\nCross-Convolution Attention module, information propagation\\nand feature fusion are performed on the nodes of the graph.\\n2377-3766 © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 0, 'page_label': '1'}, page_content='and feature fusion are performed on the nodes of the graph.\\n2377-3766 © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee https://www.ieee.org/publications/rights/index.html for more information.\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='436 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024\\nGraph convolution operations capture global and local point\\ncloud features, while the Cross-Convolution Attention module\\ncomputes the attention distribution between different input point\\nclouds, associating relevant information across different point\\nclouds, learning their dependencies and importance weights, and\\nassisting in matching and aligning different point clouds.\\nThe end-to-end CCAG-based point cloud registration method\\nhas the following advantages:\\nr The AGCM module incorporates shape descriptors and\\nlocal features into the model, addressing the issues of point\\ncloud data regularization, parameter selection, and design,\\nand enhancing the model’s expressive power.\\nr The Cross-Convolution Attention module, by introducing\\ncross-attention computations, models the correlation be-\\ntween point clouds and leverages convolution operations\\nto extract local features, further improving the accuracy'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='cross-attention computations, models the correlation be-\\ntween point clouds and leverages convolution operations\\nto extract local features, further improving the accuracy\\nand robustness of registration.\\nr We evaluated the proposed method on indoor, outdoor, syn-\\nthetic, and incomplete synthetic datasets, demonstrating\\nstate-of-the-art performance.\\nII. RELATED WORK\\nFeature Extraction Based on Deep Learning:In recent years,\\nwith the rapid development and advancement of deep learning,\\nnew opportunities have emerged for point cloud registration.\\nWang et al.[2] proposed a method using rotation-equivariant\\ndescriptors to address the issue of traditional point cloud regis-\\ntration methods relying on handcrafted feature descriptors. By\\nintroducing rotational equivariance in the design of feature de-\\nscriptors, this method ensures their invariance to rotational trans-\\nformations of point clouds. Speciﬁcally, the authors employed\\nspherical harmonics to represent the local geometric features'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='scriptors, this method ensures their invariance to rotational trans-\\nformations of point clouds. Speciﬁcally, the authors employed\\nspherical harmonics to represent the local geometric features\\nof point clouds and constructed rotation-equivariant descriptors\\nusing rotation-invariant spherical harmonics. Kadam et al.[3]\\npresented an unsupervised point cloud registration method based\\non R-pointhop. The method ﬁrst applies an adaptive sampling\\nstrategy to downsample the point cloud, reducing computational\\ncomplexity. Then, it computes feature descriptors of the point\\ncloud for local neighborhood matching and initial alignment.\\nNext, an iterative registration method called pointhop is used\\nto progressively improve the alignment accuracy. The pointhop\\nmethod aligns the point cloud iteratively through adaptive fea-\\nture selection and geometry-based strategies until achieving the\\nﬁnal precise registration result.\\nFurthermore, Poiesi et al.[4] proposed an improved method'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='ture selection and geometry-based strategies until achieving the\\nﬁnal precise registration result.\\nFurthermore, Poiesi et al.[4] proposed an improved method\\nfor point cloud registration by learning general and distinctive\\n3D local depth descriptors. This approach uses deep learning\\nto extract feature representations of point clouds and utilizes\\nthe learned descriptors for matching and registration, thereby\\nachieving more accurate and robust point cloud registration\\nresults. In addressing the balance between accuracy, efﬁciency,\\nand generalization, Ao et al.[5] introduced a point cloud reg-\\nistration method called BUFFER. This method leverages both\\npoint-to-point and face-to-face techniques while overcoming\\ntheir inherent shortcomings. Speciﬁcally, they ﬁrst introduced\\na point-to-point learner to enhance computational efﬁciency\\nand improve feature representation by predicting key points\\nand estimating point orientations. Subsequently, they deployed'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='a point-to-point learner to enhance computational efﬁciency\\nand improve feature representation by predicting key points\\nand estimating point orientations. Subsequently, they deployed\\na face-to-face embedder to extract efﬁcient and generic face\\nfeatures using lightweight local feature learners. Different from\\ncommonly used supervised models, Huang et al.[6] proposed an\\ninnovative unsupervised point cloud registration method. They\\nutilized a uniﬁed Gaussian mixture model to handle sampling\\nnoise and density variations, thereby enhancing the accuracy and\\nefﬁciency of point cloud registration while reducing dependence\\non supervised information.\\nEnd-to-end registration based on deep learning:With the\\nrapid development of deep learning, end-to-end point cloud\\nregistration methods based on deep learning have gradually\\nbecome a research hotspot. These methods directly learn the\\ntransformation parameters for registration from raw point cloud'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='registration methods based on deep learning have gradually\\nbecome a research hotspot. These methods directly learn the\\ntransformation parameters for registration from raw point cloud\\ndata without explicit feature extraction and matching processes.\\nBy constructing end-to-end neural network models that map\\ninput point cloud data to output transformation parameters,\\naccurate alignment of point clouds can be achieved.\\nTraditional point cloud registration methods often require\\na high degree of overlap between point clouds to establish\\ncorrespondences through matching shared features. However,\\nin low-overlap scenarios, the shared features between point\\nclouds are scarce, making it challenging for traditional meth-\\nods to achieve accurate registration. To address this issue, the\\nPredator method[7] introduced an autoencoder-based approach.\\nThe encoder part of the autoencoder is used to extract feature\\nrepresentations of the point cloud, while the decoder part is'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='Predator method[7] introduced an autoencoder-based approach.\\nThe encoder part of the autoencoder is used to extract feature\\nrepresentations of the point cloud, while the decoder part is\\nresponsible for reconstructing the input point cloud. To han-\\ndle point cloud registration in low-overlap scenarios, Predator\\nincorporates an adaptive point cloud sampling method. By dy-\\nnamically adjusting the point cloud’s sampling density during\\nthe registration process, Predator can better handle point cloud\\nregistration problems in low-overlap situations.\\nOn the other hand, traditional point cloud registration methods\\noften rely on iterative optimization processes to ﬁnd the optimal\\nregistration transformation. However, these iterative optimiza-\\ntion processes often consume a signiﬁcant amount of computa-\\ntion time and require high-quality initial alignment. To address\\nthis problem, Qin et al.[8] proposed a fast and robust point cloud\\nregistration method based on geometric transformations. This'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='tion time and require high-quality initial alignment. To address\\nthis problem, Qin et al.[8] proposed a fast and robust point cloud\\nregistration method based on geometric transformations. This\\nmethod ﬁrst computes an initial coarse alignment transformation\\nthrough feature extraction and matching. Then, a geometric\\ntransformation module is utilized to reﬁne the alignment by\\napplying nonlinear transformations based on learnable parame-\\nters, capturing the geometric relationships between point clouds.\\nFinally, through ﬁne alignment and registration evaluation, the\\nﬁnal point cloud registration result is obtained.\\nExisting point cloud descriptors rely on structural information\\nwhile ignoring texture information, yet texture information is\\ncrucial for distinguishing scene components. To address this\\nissue, Huang et al.[9], [10] proposed a method based on multi-\\nmodal fusion to generate point cloud registration descriptors that\\ntake into account both structural and texture information.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 1, 'page_label': '2'}, page_content='issue, Huang et al.[9], [10] proposed a method based on multi-\\nmodal fusion to generate point cloud registration descriptors that\\ntake into account both structural and texture information.\\nFurthermore, there are methods that leverage Transformer\\nnetworks to learn end-to-end point cloud correspondences. For\\nexample, Yew et al.[11] proposed a method using Transformer\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 2, 'page_label': '3'}, page_content='WANG et al.: CCAG: END-TO-END POINT CLOUD REGISTRATION 437\\nFig. 1. Network architecture of CCAG.\\nnetworks. This method ﬁrst encodes the input point cloud\\nthrough an encoder network, transforming the position and fea-\\nture information of the point cloud into a set of high-dimensional\\nrepresentations. Then, self-attention mechanisms are employed\\nto learn the relationships between points in the point cloud, cap-\\nturing both global and local feature associations in the encoded\\npoint cloud representation. Finally, a fully connected layer maps\\nthe point cloud representation to the prediction space of point\\ncloud correspondences. This prediction space can represent the\\nprobability of correspondence between each point in the point\\ncloud and other points. Based on the predicted correspondences,\\nan iterative optimization algorithm is used to resolve the optimal\\npoint cloud correspondences. Additionally, there are unsuper-\\nvised deep probabilistic methods, such as the method proposed'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 2, 'page_label': '3'}, page_content='an iterative optimization algorithm is used to resolve the optimal\\npoint cloud correspondences. Additionally, there are unsuper-\\nvised deep probabilistic methods, such as the method proposed\\nb yM e ie ta l .[12]. This method uses a deep neural network\\nto encode partial input point clouds into low-dimensional fea-\\nture vectors, which are then inputted into a Gaussian mixture\\nmodel. By introducing deep probabilistic models, this method\\ncan automatically learn the matching and registration process\\nfrom partial point clouds in an unsupervised manner.\\nIn summary, deep learning-based point cloud registration\\nmethods offer more accurate, robust, and adaptable solutions\\nfor various registration scenarios through feature learning and\\nend-to-end training. These methods can handle noise and incom-\\npleteness in point clouds and achieve satisfactory registration\\nresults. However, there are still challenges, such as point cloud\\nregistration in large-scale scenes and low-overlap scenarios,'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 2, 'page_label': '3'}, page_content='pleteness in point clouds and achieve satisfactory registration\\nresults. However, there are still challenges, such as point cloud\\nregistration in large-scale scenes and low-overlap scenarios,\\nwhich remain hotspots and research challenges in the ﬁeld.\\nIII. METHOD\\nOur method follows a hierarchical structure, similar to\\nD3Feat [13] and Predator[7]. The speciﬁc process is illustrated\\nin Fig.1.\\nA. Problem Setting\\nFor two point clouds deﬁned as the source point cloudP =\\n{pi ∈ R3 | i =1 ,2,...,N } and the target point cloud Q =\\n{qi ∈ R3 | i =1 ,2,...,M }, whereN and M are the number\\nof points in point cloudsP and Q, respectively, the goal of\\npoint cloud registration is to align the two point clouds using an\\nFig. 2. Structural ﬂow of the AGCM module and CCA-Net module.\\nunknown 3D rigid transformationRT = {R,T }, which con-\\nsists of a rotationR ∈ SO(3) and a translation T ∈ R3.T h e\\ntransformation matrix can be deﬁned as:\\nmin\\nR,T\\n∑\\n(pi,qi)∈ϑ\\n∥R·pi +T −qi∥2\\n2 (1)'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 2, 'page_label': '3'}, page_content='unknown 3D rigid transformationRT = {R,T }, which con-\\nsists of a rotationR ∈ SO(3) and a translation T ∈ R3.T h e\\ntransformation matrix can be deﬁned as:\\nmin\\nR,T\\n∑\\n(pi,qi)∈ϑ\\n∥R·pi +T −qi∥2\\n2 (1)\\nWhere ϑ represents the ground truth correspondences between\\nthe points inP and Q. The notation∥•∥ denotes the Euclidean\\nnorm.\\nTo address this problem, we propose an end-to-end point\\ncloud registration algorithm called CCAG, which takes a pair\\nof point clouds as input and output the correspondence between\\npoints, and estimates rigid transformations using RANSAC.\\nB. Downsampling and Feature Extraction\\nFor the denser original point clouds P ∈ RN×3 and Q ∈\\nRM×3, we utilize the KPConv module as the backbone, which\\nconsists of a series of residual modules and strided convolutions,\\nto perform downsampling and reduce the number of keypoints\\nto P′∈ RN′×3 and Q′∈ RM′×3, respectively (whereN>N ′\\nand M>M ′). Furthermore, we employ a shared encoding\\nmethod to extract relevant features, resulting inF′'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 2, 'page_label': '3'}, page_content='to P′∈ RN′×3 and Q′∈ RM′×3, respectively (whereN>N ′\\nand M>M ′). Furthermore, we employ a shared encoding\\nmethod to extract relevant features, resulting inF′\\nP′ ∈ RN′×D\\nand F′\\nQ′ ∈ RM′×D, whereD represents the feature dimension.\\nC. Overlap Attention Module\\nThe structural ﬂow of the AGCM module and CCA-Net\\nmodule in the Overlapping Attention Module is shown in Fig.2.\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 3, 'page_label': '4'}, page_content='438 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024\\nAdaptive Graph Convolution MLP:To address the irregularity\\nof point cloud data and the challenges in selecting and designing\\nnetwork structure parameters, we propose an Adaptive Graph\\nConvolutional MLP network, referred to as AGCM. Since the\\nprocessing steps for the source point cloud and the target point\\ncloud are identical, we will use the source point cloud as an\\nexample.\\nFirstly, we deﬁne the graph structureG(V,E ), where V =\\n{1,2,...,N ′}represents the set of vertices andE ⊆| V|×| V|\\nrepresents the set of edges, with each vertex corresponding to a\\npoint p′\\ni in P′.\\nNext, through the adaptive graph convolution operation, the\\nfeature information of nodes is dynamically aggregated and\\ntransformed. This operation utilizes the feature information of\\nneighboring nodes to update the feature representation of the\\ncurrent node. Then, the maxpooling operation is employed to'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 3, 'page_label': '4'}, page_content='transformed. This operation utilizes the feature information of\\nneighboring nodes to update the feature representation of the\\ncurrent node. Then, the maxpooling operation is employed to\\nextract key features. Finally, the MLP enhancement algorithm\\nis introduced to enhance the expressiveness of point cloud data\\nand enrich the features.\\n(k+1)P′= ϑm(max(Δpij ·Δfij)) (2)\\nFAGCM\\ni = γθ(cat[(0)P′,(1)P′,(2)P′]) (3)\\nWhere, Δpij =[ p′\\ni,p′\\nj −p′\\ni] is the graph vertex and its relative\\nposition, Δfij is deﬁned as[f′\\ni,f ′\\nj −f′\\ni] correspondingly, and\\ncat[•,•]is the concatenation operation.ϑm stands for pointwise\\nMLP andγθ stands for linear layer.maxstands for maxpooling.\\nCross-Convolution Attention: To enhance the feature mod-\\neling capability, context relationship learning, handling spatial\\nlocality features, and possessing strong model generalization\\nability, we propose a Cross-Convolution Attention module, re-\\nferred to as CCA-Net.\\nFirstly, we deﬁne a 4-layer multi-head cross-attention module'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 3, 'page_label': '4'}, page_content='ability, we propose a Cross-Convolution Attention module, re-\\nferred to as CCA-Net.\\nFirstly, we deﬁne a 4-layer multi-head cross-attention module\\nthat inherits the local feature information from the AGCM\\nmodule and focuses on the mutual information between two\\npoint clouds.\\nMHAttn(Q,K,V )=( Head1 ⊕···⊕ HeadH)Wo (4)\\nHeadH = Attn(QWQ\\nh ,KW K\\nh ,VW V\\nh ) (5)\\nWhere, WQ\\nh ∈ Rd×dQ,W K\\nh ∈ Rd×dK ,W V\\nh ∈ Rd×dV ,W O ∈\\nRdHead×d are the learning transformation matrix,dQ,dK,dV\\nare the dimensions of Query, Key and Value,dQ = dK = dV =\\ndHead = D/H. H is the number of heads.\\nFirstly, we deﬁne F′\\nP′ =( xP′\\n1 ,xP′\\n2 ··· xP′\\nN′) and F′\\nQ′ =\\n(xQ′\\n1 ,xQ′\\n2 ··· xQ′\\nN′) as the inputMHAttn(F′\\nP′,F ′\\nQ′,F ′\\nQ′) of the\\ncross-attention module in the i-th layer, Z′′=( zP′,\\n1\\nQ′,zP′,Q′\\n2 ··· zP′,Q′\\nN′ ) is the output matrix, and its formula\\nis as follows:\\nzP′,Q′\\ni =\\nN′\\n∑\\nj=1\\nsoft max(αCross−\\ni,j )xQ′\\nj WV,Q′\\n(6)\\nWhere, αCross−\\ni,j is the weight coefﬁcient that has not been'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 3, 'page_label': '4'}, page_content='2 ··· zP′,Q′\\nN′ ) is the output matrix, and its formula\\nis as follows:\\nzP′,Q′\\ni =\\nN′\\n∑\\nj=1\\nsoft max(αCross−\\ni,j )xQ′\\nj WV,Q′\\n(6)\\nWhere, αCross−\\ni,j is the weight coefﬁcient that has not been\\nnormalized, and its deﬁnition is as follows:\\nαCross−\\ni,j = 1√dhead\\n(xQ′\\ni WQ,Q′\\n)(xP′\\nj WK,P′\\n)T (7)\\nNext, convolution is used to further capture the local and\\nglobal features of the point cloud. Its deﬁnition is as follows:\\nFinally, the co-global context information between the two\\npoint clouds is output, which is deﬁned as follows:\\nFDSConv\\ni = F′\\nP′ +DSConv(zP′,Q′\\ni ) (8)\\nWhere, DSConv represents depthwise separable convolution,\\nwhich consists of two components: Depthwise Convolution and\\nPointwise Convolution. DSConv=PWConv (DepConv(•)), De-\\npConv, PWConv represent depthwise convolution and pointwise\\nconvolution respectively.\\nFCCA−Net\\ni = FDSConv\\ni +MLP(FDSConv\\ni ) (9)\\nD. Decoder\\nThe module is conﬁgured in the usual manner, with a 3-layer'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 3, 'page_label': '4'}, page_content='convolution respectively.\\nFCCA−Net\\ni = FDSConv\\ni +MLP(FDSConv\\ni ) (9)\\nD. Decoder\\nThe module is conﬁgured in the usual manner, with a 3-layer\\nnetwork structure that mainly includes upsampling, linear trans-\\nformation, and skip connections.\\nE. Loss Function\\nThe CCAG network we proposed is trained end-to-end and\\nsupervised using ground truth. The speciﬁc loss function is as\\nfollows:\\nFeature Loss:Similar to the D3Feat and Predator approaches,\\nwe use a circle loss function to evaluate the feature loss and\\nconstrain the point-wise feature descriptors during the training\\nof 3D point clouds. The circle loss function is deﬁned as follows:\\nLP\\nFL = 1\\nNP\\nNP∑\\ni=1\\nlog\\n⎡\\n⎣1+\\n∑\\nj∈εp\\necβj\\np(dj\\ni−Δp) •\\n∑\\nk∈εn\\neλβk\\np(Δn−dk\\ni )\\n⎤\\n⎦\\n(10)\\nWhere, dj\\ni represents the Euclidean distance between features,\\ndj\\ni = ∥fpi −fqj ∥2. εp and εn respectively represent the match-\\ning and unmatching points of the point setPRS (random sam-\\npling points of the source point cloud), i.e., the positive and neg-'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 3, 'page_label': '4'}, page_content='dj\\ni = ∥fpi −fqj ∥2. εp and εn respectively represent the match-\\ning and unmatching points of the point setPRS (random sam-\\npling points of the source point cloud), i.e., the positive and neg-\\native areas.Δpand Δnrepresent positive and negative regions,\\nrespectively. λ stands for predeﬁned parameters. Similarly, the\\nfeature lossLP\\nFL of the target point cloud is also calculated in\\nthe same way, so the total feature lossLFL = 1\\n2(LP\\nFL +LQ\\nFL).\\nOverlap Loss: We use a binary cross-entropy loss function\\nfor supervised training, which is deﬁned as follows:\\nLP\\nOL = 1\\nN\\nN∑\\ni=1\\nOlabel\\npi log(Opi)+(1 −Olabel\\npi )log(1 −Opi)\\n(11)\\nWhere, Olabel\\npi represents the overlap scores of ground truth at\\npoint pi, which is deﬁned as follows:\\nOlabel\\npi =\\n{1,\\n\\ued79\\ued79TGT\\nP,Q(pi)−NN(TGT\\nP,Q(pi),Q)\\n\\ued79\\ued79<τ 1\\n0,otherwise (12)\\nWhere TGT\\nP,Q represents the ground truth rigid transformation\\nbetween the overlapping point clouds, andNN represents the\\nnearest neighbor.τ1 represents the overlap threshold. Similarly,'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 3, 'page_label': '4'}, page_content='Where TGT\\nP,Q represents the ground truth rigid transformation\\nbetween the overlapping point clouds, andNN represents the\\nnearest neighbor.τ1 represents the overlap threshold. Similarly,\\nthe overlap lossLQ\\nOL of the target point cloud is calculated in\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 4, 'page_label': '5'}, page_content='WANG et al.: CCAG: END-TO-END POINT CLOUD REGISTRATION 439\\nthe same way. Therefore, the total overlap loss is deﬁned as\\nLOL = 1\\n2(LP\\nOL +LQ\\nOL).\\nTo sum up, the overall loss function isL = LFL +LOL.\\nIV . EXPERIMENTS\\nA. Implementation Details and Evaluation Metrics\\nOur method is implemented using PyTorch[14] and trained\\non an Nvidia RTX 4090, Intel(R) Core(TM) i7-13700KF CPU\\n@ 3.40 GHz, 128 GB RAM.\\nFollowing the evaluation metrics used in Predator [7],\\nREGTR [11], and GMCNet [15], we evaluate our models\\non the datasets using Registration Recall (RR), Relative Ro-\\ntation Error (RRE), and Relative Translation Error (RTE).\\nIn addition, we use the modiﬁed Chamfer Distance (CD) to\\nevaluate the ModelNet40 dataset, and RMSE to evaluate the\\nMVP-RG dataset. The deﬁnitions of RRE, RTE, CD are as\\nfollows:\\nRTE =\\n\\ued79\\ued79t−tGT\\ued79\\ued79\\n2 (13)\\nRRE = arccos\\n(trace(RTRGT)−1\\n2\\n)\\n(14)\\nCD(P,Q)= 1\\n|P|\\n∑\\np∈P\\nmin\\nq∈Qraw\\n\\ued79\\ued79TGT\\nP,Q(p)−q\\n\\ued79\\ued792\\n2 (15)\\n+ 1\\n|Q|\\n∑\\nq∈Q\\nmin\\np∈Praw\\n\\ued79\\ued79q −TGT\\nP,Q(p)\\n\\ued79\\ued792\\n2 (16)'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 4, 'page_label': '5'}, page_content='follows:\\nRTE =\\n\\ued79\\ued79t−tGT\\ued79\\ued79\\n2 (13)\\nRRE = arccos\\n(trace(RTRGT)−1\\n2\\n)\\n(14)\\nCD(P,Q)= 1\\n|P|\\n∑\\np∈P\\nmin\\nq∈Qraw\\n\\ued79\\ued79TGT\\nP,Q(p)−q\\n\\ued79\\ued792\\n2 (15)\\n+ 1\\n|Q|\\n∑\\nq∈Q\\nmin\\np∈Praw\\n\\ued79\\ued79q −TGT\\nP,Q(p)\\n\\ued79\\ued792\\n2 (16)\\nWhere, tGT and RGT represent the ground-truth rotational error\\nand translational error, respectively.\\nRegistration recall is deﬁned as the root mean square error of\\nthe transformation being less than 0.2 m. The formula is shown\\nbelow:\\nRMSE =\\n\\ued6a\\ued6b\\ued6b√ 1⏐⏐CGT\\nij\\n⏐⏐\\n∑\\n(p,q)∈CGT\\nij\\n\\ued79\\ued79\\ued79TGT\\nP,Q(p)−q\\n\\ued79\\ued79\\ued79\\n2\\n2\\n(17)\\nWhere, CGT\\nij represents the set of ground-truth correspondences.\\nNote that, unlike the RMSE in registration recall, where only\\nmatched points are considered, in the MVP-RG dataset, all\\npoints are involved in the computation. Therefore, to differenti-\\nate it from (18), we deﬁne the RMSE for the MVP-RG dataset\\nas follows:\\nLRMSE = 1\\nN\\nN∑\\ni=1\\n\\ued79\\ued79TGT(pi)−T(pi)\\n\\ued79\\ued79\\n2 (18)\\nB. 3DMatch and 3DLoMatch\\nDataset: The 3DMatch dataset contains real indoor data from'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 4, 'page_label': '5'}, page_content='as follows:\\nLRMSE = 1\\nN\\nN∑\\ni=1\\n\\ued79\\ued79TGT(pi)−T(pi)\\n\\ued79\\ued79\\n2 (18)\\nB. 3DMatch and 3DLoMatch\\nDataset: The 3DMatch dataset contains real indoor data from\\n62 scenarios, with 46 scenes for training, 8 for validation, and\\n8 for testing. We ﬁrst pretrain our model using the Predator\\nmethod and then evaluate it on the 3DMatch and 3DLoMatch\\ndatasets. The overlapping areas of the 3DMatch and 3DLoMatch\\ndatasets are greater than 30% and between 10% and 30%,\\nrespectively.\\nTABLE I\\nPERFORMANCE ON 3DMATCH AND3DLOMATCH DATASETS\\nFig. 3. Registration visualization on 3DMatch, 3DLoMatch.\\nBaselines: We compare our results with state-of-the-art meth-\\nods such as FCGF[16], D3Feat[13], Predator[7],O M N e t[17],\\nCoFiNet [18], GeoTrans[8], REGTR[11], UDPReg[12], Lep-\\nard [19],M A C[20] and SC2 -PCR++[21].\\nRegistration Results:The performance comparison of differ-\\nent algorithms is shown in TableI, where the best-performing al-\\ngorithms are indicated in bold. The data in the table are obtained'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 4, 'page_label': '5'}, page_content='Registration Results:The performance comparison of differ-\\nent algorithms is shown in TableI, where the best-performing al-\\ngorithms are indicated in bold. The data in the table are obtained\\nfrom REGTR[11] and UDPReg[12]. Additionally, Fig.3 illus-\\ntrates the registration results on the 3DMatch and 3DLoMatch\\ndatasets. In the testing on the 3DMatch and 3DLoMatch datasets,\\nour algorithm has demonstrated overall superior performance.\\nIt is noteworthy that in low-overlap validation, our algorithm\\nhas shown a remarkable improvement in RR compared to the\\nMAC [20] and SC2 -PCR++[21] algorithms, with increases of\\n16.3% and 15.1%, respectively.\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 5, 'page_label': '6'}, page_content='440 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024\\nTABLE II\\nFMR ON 3DMATCH AND3DLOMATCH DATASETS\\nTABLE III\\nPERFORMANCE ON ODOMETRY KITTI DATASET\\nFollowing Predator [7], we evaluated the Feature Match-\\ning Recall (FMR). In the validation experiments presented in\\nTable II , our algorithm demonstrated overall strong perfor-\\nmance.\\nC. Odometry KITTI\\nDataset: The Odometry KITTI dataset consists of large-scale\\nLiDAR scanning data from 11 scenarios, with scenarios 0–5\\nused for training, scenarios 6–7 for validation, and scenarios\\n8–10 for testing.\\nBaselines: We select FCGF[16], D3Feat[13], Predator[7],\\nCoFiNet [18], GeoTrans[8],M A C[20] and SC2 -PCR++[21]\\nas our baseline algorithms.\\nRegistration Results:The comparison of different algorithms\\ni ss h o w ni nT a b l eIII, where the best-performing algorithms are\\nindicated in bold. Additionally, Fig.4 illustrates the registration\\nresults on the KITTI Odometry dataset. In the testing on the'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 5, 'page_label': '6'}, page_content='i ss h o w ni nT a b l eIII, where the best-performing algorithms are\\nindicated in bold. Additionally, Fig.4 illustrates the registration\\nresults on the KITTI Odometry dataset. In the testing on the\\nKITTI Odometry dataset, our algorithm achieves the highest\\naverage registration recall. Moreover, we also achieve the lowest\\nRTE and RRE scores.\\nD. Modelnet\\nData: The ModelNet40 dataset is a synthetic dataset com-\\nposed of computer-aided design (CAD) models, consisting of\\n5112 samples for training, 1202 samples for validation, and 1266\\nsamples for testing. We follow the training approach of RPM-Net\\nand Predator and then evaluate our model on the ModelNet40\\nand ModelLoNet40 datasets. (The average overlap regions for\\nModelNet40 and ModelLoNet40 datasets are greater than 73.5%\\nand 53.6%, respectively.)\\nFig. 4. Registration visualization on Odometry KITTI.\\nTABLE IV\\nPERFORMANCE ON MODELNET AND MODELLONET DATASETS\\nBaselines: Based on the results of RPM-Net and Predator,'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 5, 'page_label': '6'}, page_content='and 53.6%, respectively.)\\nFig. 4. Registration visualization on Odometry KITTI.\\nTABLE IV\\nPERFORMANCE ON MODELNET AND MODELLONET DATASETS\\nBaselines: Based on the results of RPM-Net and Predator,\\nwe select several state-of-the-art algorithms, including RPM-\\nNet [24], Predator [7],O M N e t[17], REGTR [11] and UD-\\nPReg [12] as our baselines.\\nRegistration Results:The comparison of different algorithms\\nis shown in Table IV, where the best-performing algorithm\\nis indicated in bold. The data for the table is sourced from\\nREGTR. Additionally, Fig. 5 presents the registration results\\non the ModelNet/ModelLoNet datasets. Our algorithm exhibits\\ngood performance in various evaluations during the testing of\\nthe ModelNet and ModelLoNet datasets.\\nE. MVP-RG\\nData: The MVP-RG dataset is a synthetic dataset of incom-\\nplete point clouds, consisting of 6400 pairs for training and 1200\\npairs for testing. We trained our model using an approach similar\\nto Predator and evaluated it on the MVP-RG dataset.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 5, 'page_label': '6'}, page_content='plete point clouds, consisting of 6400 pairs for training and 1200\\npairs for testing. We trained our model using an approach similar\\nto Predator and evaluated it on the MVP-RG dataset.\\nBaselines: We selected several state-of-the-art algorithms,\\nincluding DCP[25], RPM-Net[24],G M C N e t[15],I D A M[26],\\nDeepGMR [27], Predator and DSMNet[28] as our baselines.\\nRegistration Results:The comparison of different algorithms\\nis shown in TableV, where the best performing algorithm is\\nhighlighted in bold, and the data is sourced from GMCNet[15].\\nAdditionally, Fig. 6 illustrates the registration results on the\\nMVP-RG dataset. In the testing phase on the MVP-RG dataset,\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 6, 'page_label': '7'}, page_content='WANG et al.: CCAG: END-TO-END POINT CLOUD REGISTRATION 441\\nFig. 5. Registration visualization on ModelNet and ModelLoNet.\\nTABLE V\\nPERFORMANCE ON MVP-RG DATASET\\nour algorithm outperforms other methods in all evaluation met-\\nrics.\\nF . Ablation Study\\nImportance of Individual Modules:To validate the effective-\\nness of selected modules in our model, we conducted ablation\\nexperiments on the 3DMatch/3DLoMatch datasets.\\nFrom Table VI, it can be seen that compared to the other\\nfour individual modules, our proposed CCAG algorithm demon-\\nstrates the best performance.\\nLoss Functions: To evaluate the impact of different loss\\nfunctions and their combinations on the model, we conducted\\nexperiments on the 3DMatch/3DLoMatch datasets. Here, OL\\nrepresents the overlap loss, FL represents the feature loss, and\\nML represents the matching loss.\\nFrom TableVII, it can be observed that all three loss functions\\ncan improve the registration accuracy, with the best performance'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 6, 'page_label': '7'}, page_content='ML represents the matching loss.\\nFrom TableVII, it can be observed that all three loss functions\\ncan improve the registration accuracy, with the best performance\\nFig. 6. Registration visualization on MVP-RG.\\nTABLE VI\\nABLATION OFDIFFERENT MODULES ON THE3DMATCH,3 DLOMATCH\\nDATASET\\nTABLE VII\\nABLATION OFDIFFERENT LOSS FUNCTIONS ON THE3DMATCH,\\n3DLOMATCH DATASET\\nachieved when the three loss functions are combined. Among\\nthem, in terms of single loss functions and pairwise combina-\\ntions, the feature loss function or the combination involving the\\nfeature loss function shows the best performance. The overlap\\nloss function performs well, while the matching loss function\\nshows relatively poorer performance.\\nV. CONCLUSION\\nIn response to the non-structural and incomplete nature of\\npoint clouds, as well as the challenges faced by traditional point\\ncloud registration methods in handling low overlap and large-\\nscale data, we propose an end-to-end point cloud registration'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 6, 'page_label': '7'}, page_content='point clouds, as well as the challenges faced by traditional point\\ncloud registration methods in handling low overlap and large-\\nscale data, we propose an end-to-end point cloud registration\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='442 IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 9, NO. 1, JANUARY 2024\\nalgorithm based on CCAG. In the CCAG algorithm, the AGCM\\nmodule converts point clouds into graph structures and adap-\\ntively learns convolutional kernels based on the characteristics\\nof point clouds, enabling effective capture of local and global\\nfeatures. The CCA-Net module introduces depth-wise separa-\\nble convolution and cross-convolution attention computation to\\nmodel the correlations between point clouds and extract local\\nfeatures of overlapping point clouds, thereby further improving\\nthe accuracy and robustness of registration.\\nCompared to traditional registration methods, our algorithm\\ncan better handle the geometric structure and local features of\\npoint clouds, providing more accurate results for point cloud\\nregistration tasks. This approach offers an effective solution for\\npoint cloud processing and the ﬁeld of computer vision in point\\ncloud registration tasks.\\nREFERENCES'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='registration tasks. This approach offers an effective solution for\\npoint cloud processing and the ﬁeld of computer vision in point\\ncloud registration tasks.\\nREFERENCES\\n[1] J. Li, P. Shi, Q. Hu, and Y . Zhang, “QGORE: Quadratic-time guaranteed\\noutlier removal for point cloud registration,”IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 45, no. 9, pp. 11136–11151, Sep. 2023.\\n[2] H. Wang, Y . Liu, Z. Dong, and W. Wang, “You only hypothesize once:\\nPoint cloud registration with rotation-equivariant descriptors,” inProc.\\n30th ACM Int. Conf. Multimedia, 2022, pp. 1630–1641.\\n[3] P. Kadam, M. Zhang, S. Liu, and C.-C. J. Kuo, “R-pointhop: A green,\\naccurate, and unsupervised point cloud registration method,”IEEE Trans.\\nImage Process., vol. 31, pp. 2710–2725, 2022.\\n[4] F. Poiesi and D. Boscaini, “Learning general and distinctive 3D local deep\\ndescriptors for point cloud registration,”IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 45, no. 3, pp. 3979–3985, Mar. 2023.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='descriptors for point cloud registration,”IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 45, no. 3, pp. 3979–3985, Mar. 2023.\\n[5] S. Ao, Q. Hu, H. Wang, K. Xu, and Y . Guo, “Buffer: Balancing accu-\\nracy, efﬁciency, and generalizability in point cloud registration,” inProc.\\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2023, pp. 1255–1264.\\n[6] X. Huang, S. Li, Y . Zuo, Y . Fang, J. Zhang, and X. Zhao, “Unsupervised\\npoint cloud registration by learning uniﬁed Gaussian mixture models,”\\nIEEE Robot. Automat. Lett., vol. 7, no. 3, pp. 7028–7035, Jul. 2022.\\n[7] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, and K. Schindler, “Predator:\\nRegistration of 3D point clouds with low overlap,” inProc. IEEE/CVF\\nConf. Comput. Vis. Pattern Recognit., 2021, pp. 4267–4276.\\n[8] Z. Qin, H. Yu, C. Wang, Y . Guo, Y . Peng, and K. Xu, “Geometric trans-\\nformer for fast and robust point cloud registration,” inProc. IEEE/CVF\\nConf. Comput. Vis. Pattern Recognit., 2022, pp. 11143–11152.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='former for fast and robust point cloud registration,” inProc. IEEE/CVF\\nConf. Comput. Vis. Pattern Recognit., 2022, pp. 11143–11152.\\n[9] X. Huang, W. Qu, Y . Zuo, Y . Fang, and X. Zhao, “IMFNet: Interpretable\\nmultimodal fusion for point cloud registration,”IEEE Robot. Automat.\\nLett., vol. 7, no. 4, pp. 12323–12330, Oct. 2022.\\n[10] M. Yuan, X. Huang, K. Fu, Z. Li, and M. Wang, “Boosting 3D point cloud\\nregistration by transferring multi-modality knowledge,” inProc. IEEE Int.\\nConf. Robot. Automat., 2023, pp. 11734–11741.\\n[11] Z. J. Yew and G. H. Lee, “RegTR: End-to-end point cloud correspon-\\ndences with transformers,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\\nRecognit., 2022, pp. 6677–6686.\\n[12] G. Mei et al., “Unsupervised deep probabilistic approach for partial\\npoint cloud registration,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\\nRecognit., 2023, pp. 13611–13620.\\n[13] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, “D3Feat: Joint'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='point cloud registration,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\\nRecognit., 2023, pp. 13611–13620.\\n[13] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, “D3Feat: Joint\\nlearning of dense detection and description of 3D local features,” inProc.\\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 6359–6367.\\n[14] A. Paszke et al., “PyTorch: An imperative style, high-performance deep\\nlearning library,” inProc. 33rd Int. Conf. Neural Inf. Process. Syst., 2019,\\npp. 8026–8037.\\n[15] L. Pan, Z. Cai, and Z. Liu, “Robust partial-to-partial point cloud registra-\\ntion in a full range,” 2021,arXiv:2111.15606.\\n[16] C. Choy, J. Park, and V . Koltun, “Fully convolutional geometric features,”\\nin Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 8958–8966.\\n[17] H. Xu, S. Liu, G. Wang, G. Liu, and B. Zeng, “OMNet: Learning\\noverlapping mask for partial-to-partial point cloud registration,” inProc.\\nIEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 3132–3141.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='[17] H. Xu, S. Liu, G. Wang, G. Liu, and B. Zeng, “OMNet: Learning\\noverlapping mask for partial-to-partial point cloud registration,” inProc.\\nIEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 3132–3141.\\n[18] H. Yu, F. Li, M. Saleh, B. Busam, and S. Ilic, “CoFinet: Reliable coarse-\\nto-ﬁne correspondences for robust point cloud registration,” inProc. Adv.\\nNeural Inf. Process. Syst., 2021, pp. 23872–23884.\\n[19] Y . Li and T. Harada, “Lepard: Learning partial point cloud matching in rigid\\nand deformable scenes,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\\nRecognit., 2022, pp. 5554–5564.\\n[20] X. Zhang, J. Yang, S. Zhang, and Y . Zhang, “3D registration with maximal\\ncliques,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2023,\\npp. 17745–17754.\\n[21] Z. Chen, K. Sun, F. Yang, L. Guo, and W. Tao, “SC2-PCr : Rethinking the\\ngeneration and selection for efﬁcient and robust point cloud registration,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 10, pp. 12358–12376,\\nOct. 2023.'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='generation and selection for efﬁcient and robust point cloud registration,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 10, pp. 12358–12376,\\nOct. 2023.\\n[22] H. Yu et al., “Rotation-invariant transformer for point cloud match-\\ning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2023,\\npp. 5384–5393.\\n[23] G. Mei, X. Huang, L. Yu, J. Zhang, and M. Bennamoun, “COTReg:\\nCoupled optimal transport based point cloud registration,” 2021,\\narXiv:2112.14381.\\n[24] Z. J. Yew and G. H. Lee, “RPM-Net: Robust point matching using learned\\nfeatures,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020,\\npp. 11824–11833.\\n[25] Y . Wang and J. M. Solomon, “Deep closest point: Learning representations\\nfor point cloud registration,” inProc. IEEE/CVF Int. Conf. Comput. Vis.,\\n2019, pp. 3523–3532.\\n[26] J. Li, C. Zhang, Z. Xu, H. Zhou, and C. Zhang, “Iterative distance-aware\\nsimilarity matrix convolution with mutual-supervised point elimination'),\n",
       " Document(metadata={'source': 'CCAG_End-to-End_Point_Cloud_Registration.pdf', 'page': 7, 'page_label': '8'}, page_content='2019, pp. 3523–3532.\\n[26] J. Li, C. Zhang, Z. Xu, H. Zhou, and C. Zhang, “Iterative distance-aware\\nsimilarity matrix convolution with mutual-supervised point elimination\\nfor efﬁcient point cloud registration,” inProc. Comput. Vis. ECCV: 16th\\nEur. Conf., 2020, pp. 378–394.\\n[27] W. Yuan, B. Eckart, K. Kim, V . Jampani, D. Fox, and J. Kautz, “Deep-\\nGMR: Learning latent Gaussian mixture models for registration,” inProc.\\nComput. Vis., 16th Eur. Conf., 2020, pp. 733–750.\\n[28] C. Qiu, Z. Wang, X. Lin, Y . Zang, C. Wang, and W. Liu, “DSMNet: Deep\\nhigh-precision 3D surface modeling from sparse point cloud frames,”IEEE\\nGeosci. Remote Sens. Lett., vol. 20, pp. 1–5, 2023.\\nAuthorized licensed use limited to: VTU Consortium. Downloaded on November 12,2024 at 07:22:36 UTC from IEEE Xplore.  Restrictions apply.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunk_docs = text_splitter.split_documents(docs)\n",
    "chunk_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Embeddings and Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x160273d6ad0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "# from langchain.embeddings import AzureOpenAIEmbeddings (Depricated package)\n",
    "# from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(chunk_docs, AzureOpenAIEmbeddings(model='gpt-text-embedding-3-large'))\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANG et al.: CCAG: END-TO-END POINT CLOUD REGISTRATION 437\n",
      "Fig. 1. Network architecture of CCAG.\n",
      "networks. This method ﬁrst encodes the input point cloud\n",
      "through an encoder network, transforming the position and fea-\n",
      "ture information of the point cloud into a set of high-dimensional\n",
      "representations. Then, self-attention mechanisms are employed\n",
      "to learn the relationships between points in the point cloud, cap-\n",
      "turing both global and local feature associations in the encoded\n",
      "point cloud representation. Finally, a fully connected layer maps\n",
      "the point cloud representation to the prediction space of point\n",
      "cloud correspondences. This prediction space can represent the\n",
      "probability of correspondence between each point in the point\n",
      "cloud and other points. Based on the predicted correspondences,\n",
      "an iterative optimization algorithm is used to resolve the optimal\n",
      "point cloud correspondences. Additionally, there are unsuper-\n",
      "vised deep probabilistic methods, such as the method proposed\n"
     ]
    }
   ],
   "source": [
    "query=\"This method first encodes the input point cloud through an encoder network, transforming the position and feature informationofthepointcloudintoasetofhigh-dimensional representations.\"\n",
    "retrieved_result = db.similarity_search(query)\n",
    "print(retrieved_result[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
