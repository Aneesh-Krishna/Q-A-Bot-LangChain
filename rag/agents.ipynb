{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG With multiple data-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wikipedia tool\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000257F4B71990>, search_kwargs={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load\n",
    "loader=WebBaseLoader('https://docs.smith.langchain.com/')\n",
    "\n",
    "docs=loader.load()\n",
    "\n",
    "# Split to chunks\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunk_docs=text_splitter.split_documents(docs)\n",
    "\n",
    "# Vector DB and Vector Stores\n",
    "db=FAISS.from_documents(chunk_docs,AzureOpenAIEmbeddings(model=\"gpt-text-embedding-3-large\"))\n",
    "# db\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='langsmith_search', description='Search for information about langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000257E3ADEDE0>, retriever=VectorStoreRetriever(tags=['FAISS', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000257F4B71990>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000257E3E34220>, retriever=VectorStoreRetriever(tags=['FAISS', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000257F4B71990>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create retriever tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retrieval_tool = create_retriever_tool(retriever, \"langsmith_search\",\"Search for information about langsmith\")\n",
    "retrieval_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arxiv tool\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv,retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm=AzureChatOpenAI(model='gpt-4o-mini', temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt template\n",
    "from langchain import hub #Get a prebuilt prompt from the hub\n",
    "\n",
    "prompt = hub.pull('hwchase17/openai-functions-agent')\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000257E302E7A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000257E302E7A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000257B33E34D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000257B2327E50>, root_client=<openai.lib.azure.AzureOpenAI object at 0x00000257F4CA9CD0>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x00000257B2BED0D0>, model_name='gpt-4o-mini', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), disabled_params={'parallel_tool_calls': None}, azure_endpoint='https://ip360-ai.openai.azure.com/', openai_api_version='2024-04-01-preview', openai_api_type='Azure'), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'Search for information about langsmith', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "agent = create_openai_tools_agent(llm,tools,prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Azure OpenAI Embedding'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Mistral AI\n",
      "Summary: Mistral AI, headquartered in Paris, France specializes in artificial intelligence (AI) products and focuses on open-weight large language models, (LLMs). Founded in April 2023 by former engineers from Google DeepMind and Met\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'Azure OpenAI Embedding'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mPythonTypeScriptimport openaifrom langsmith import wrappers, traceable# Auto-trace LLM calls in-contextclient = wrappers.wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-4o-mini\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-4o-mini\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there!\n",
      "\n",
      "PythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\n",
      "2. Create an API key​\n",
      "To create an API key head to the Settings page. Then click Create API Key.\n",
      "3. Set up your environment​\n",
      "Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key>export OPENAI_API_KEY=<your-openai-api-key>\n",
      "4. Log your first trace​\n",
      "We provide multiple ways to log traces to LangSmith. Below, we'll highlight\n",
      "how to use traceable(). See more on the Annotate code for tracing page.\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "LangSmith is a standalone platform that can be used on it's own no matter how you're creating your LLM applicatons.\n",
      "In this tutorial, we'll walk you though logging your first trace in LangSmith using the LangSmith SDK and running an evaluation to measure the performance of your application. This example uses the OpenAI API, however you can use your provider of choice.\n",
      "1. Install LangSmith​\n",
      "PythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\n",
      "2. Create an API key​\n",
      "To create an API key head to the Settings page. Then click Create API Key.\n",
      "3. Set up your environment​\n",
      "\n",
      "\"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { response: \"Welcome to LangSmith\" },    { response: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluator(s)const exactMatch = async ({ outputs, referenceOutputs }: {  outputs?: Record<string, any>;  referenceOutputs?: Record<string, any>;}): Promise<EvaulationResult> => {  return {    key: \"exact_match\",    score: outputs?.response === referenceOutputs?.response,  };};// Run the evaluationconst experimentResults = await evaluate(  (inputs: { postfix: string }) => ({ response: `Welcome ${inputs.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: { version: \"1.0.0\", revision_id: \"beta\" },    maxConcurrency: 4,  });\u001b[0m\u001b[32;1m\u001b[1;3mTo use Azure OpenAI Embedding, you can follow these steps:\n",
      "\n",
      "### 1. Install Required Libraries\n",
      "You need to install the necessary libraries. If you're using Python, you can do this via pip:\n",
      "\n",
      "```bash\n",
      "pip install -U langsmith openai\n",
      "```\n",
      "\n",
      "For TypeScript, you can use:\n",
      "\n",
      "```bash\n",
      "yarn add langsmith openai\n",
      "```\n",
      "\n",
      "### 2. Create an API Key\n",
      "- Go to the Azure portal and navigate to the OpenAI service.\n",
      "- Create an API key by heading to the Settings page and clicking on \"Create API Key\".\n",
      "\n",
      "### 3. Set Up Your Environment\n",
      "You need to set up your environment variables to include your API keys. In your terminal, you can set them like this:\n",
      "\n",
      "```bash\n",
      "export LANGCHAIN_TRACING_V2=true\n",
      "export LANGCHAIN_API_KEY=<your-langchain-api-key>\n",
      "export OPENAI_API_KEY=<your-openai-api-key>\n",
      "```\n",
      "\n",
      "### 4. Use the Azure OpenAI Embedding in Your Code\n",
      "Here is an example of how to use Azure OpenAI Embedding in Python:\n",
      "\n",
      "```python\n",
      "import openai\n",
      "from langsmith import wrappers, traceable\n",
      "\n",
      "# Auto-trace LLM calls in-context\n",
      "client = wrappers.wrap_openai(openai.Client())\n",
      "\n",
      "@traceable  # Auto-trace this function\n",
      "def pipeline(user_input: str):\n",
      "    result = client.chat.completions.create(\n",
      "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
      "        model=\"gpt-4o-mini\"\n",
      "    )\n",
      "    return result.choices[0].message.content\n",
      "\n",
      "# Example usage\n",
      "response = pipeline(\"Hello, world!\")\n",
      "print(response)  # Output: Hello there! How can I assist you today?\n",
      "```\n",
      "\n",
      "For TypeScript, the code would look like this:\n",
      "\n",
      "```typescript\n",
      "import { OpenAI } from \"openai\";\n",
      "import { traceable } from \"langsmith/traceable\";\n",
      "import { wrapOpenAI } from \"langsmith/wrappers\";\n",
      "\n",
      "// Auto-trace LLM calls in-context\n",
      "const client = wrapOpenAI(new OpenAI());\n",
      "\n",
      "// Auto-trace this function\n",
      "const pipeline = traceable(async (user_input) => {\n",
      "    const result = await client.chat.completions.create({\n",
      "        messages: [{ role: \"user\", content: user_input }],\n",
      "        model: \"gpt-4o-mini\",\n",
      "    });\n",
      "    return result.choices[0].message.content;\n",
      "});\n",
      "\n",
      "// Example usage\n",
      "const response = await pipeline(\"Hello, world!\");\n",
      "console.log(response);  // Output: Hello there!\n",
      "```\n",
      "\n",
      "### 5. Log Your First Trace\n",
      "You can log traces using the `traceable()` function provided by LangSmith. This allows you to monitor and evaluate the performance of your application.\n",
      "\n",
      "### Conclusion\n",
      "These steps will help you set up and use Azure OpenAI Embedding in your applications. Make sure to replace placeholders with your actual API keys and adjust the model name as needed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How to use AzureOpenAIEmbedding?',\n",
       " 'output': 'To use Azure OpenAI Embedding, you can follow these steps:\\n\\n### 1. Install Required Libraries\\nYou need to install the necessary libraries. If you\\'re using Python, you can do this via pip:\\n\\n```bash\\npip install -U langsmith openai\\n```\\n\\nFor TypeScript, you can use:\\n\\n```bash\\nyarn add langsmith openai\\n```\\n\\n### 2. Create an API Key\\n- Go to the Azure portal and navigate to the OpenAI service.\\n- Create an API key by heading to the Settings page and clicking on \"Create API Key\".\\n\\n### 3. Set Up Your Environment\\nYou need to set up your environment variables to include your API keys. In your terminal, you can set them like this:\\n\\n```bash\\nexport LANGCHAIN_TRACING_V2=true\\nexport LANGCHAIN_API_KEY=<your-langchain-api-key>\\nexport OPENAI_API_KEY=<your-openai-api-key>\\n```\\n\\n### 4. Use the Azure OpenAI Embedding in Your Code\\nHere is an example of how to use Azure OpenAI Embedding in Python:\\n\\n```python\\nimport openai\\nfrom langsmith import wrappers, traceable\\n\\n# Auto-trace LLM calls in-context\\nclient = wrappers.wrap_openai(openai.Client())\\n\\n@traceable  # Auto-trace this function\\ndef pipeline(user_input: str):\\n    result = client.chat.completions.create(\\n        messages=[{\"role\": \"user\", \"content\": user_input}],\\n        model=\"gpt-4o-mini\"\\n    )\\n    return result.choices[0].message.content\\n\\n# Example usage\\nresponse = pipeline(\"Hello, world!\")\\nprint(response)  # Output: Hello there! How can I assist you today?\\n```\\n\\nFor TypeScript, the code would look like this:\\n\\n```typescript\\nimport { OpenAI } from \"openai\";\\nimport { traceable } from \"langsmith/traceable\";\\nimport { wrapOpenAI } from \"langsmith/wrappers\";\\n\\n// Auto-trace LLM calls in-context\\nconst client = wrapOpenAI(new OpenAI());\\n\\n// Auto-trace this function\\nconst pipeline = traceable(async (user_input) => {\\n    const result = await client.chat.completions.create({\\n        messages: [{ role: \"user\", content: user_input }],\\n        model: \"gpt-4o-mini\",\\n    });\\n    return result.choices[0].message.content;\\n});\\n\\n// Example usage\\nconst response = await pipeline(\"Hello, world!\");\\nconsole.log(response);  // Output: Hello there!\\n```\\n\\n### 5. Log Your First Trace\\nYou can log traces using the `traceable()` function provided by LangSmith. This allows you to monitor and evaluate the performance of your application.\\n\\n### Conclusion\\nThese steps will help you set up and use Azure OpenAI Embedding in your applications. Make sure to replace placeholders with your actual API keys and adjust the model name as needed.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agent executor to run the agent\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "\n",
    "# agent_executor.invoke({\"input\":\"What is LangChain?\"})\n",
    "# agent_executor.invoke({\"input\":\"What is the paper 1605.08386 about?\"})\n",
    "agent_executor.invoke({\"input\":\"How to use AzureOpenAIEmbedding?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
